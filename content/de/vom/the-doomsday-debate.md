---
about:
- vectors-of-mind
- blog-archive
author: Andrew Cutler
date: '2025-07-04'
description: Uno de los temas con la peor relación señal-ruido es la perspectiva del
  apocalipsis de la IA. Requiere razonar bajo incertidumbre sobre inteligencia, álgebra
  lineal, política, conciencia y moralidad—mos...
draft: false
keywords:
- vectors-of-mind
- doomsday
- debate
lang: es
lastmod: '2025-07-13'
license: https://creativecommons.org/licenses/by-sa/4.0/
original_id: '145682175'
original_url: https://www.vectorsofmind.com/p/the-doomsday-debate
quality: 6
slug: the-doomsday-debate
tags: []
title: El Debate del Apocalipsis
translation_model: gpt-4o
---

*From [Vectors of Mind](https://www.vectorsofmind.com/p/the-doomsday-debate) - Bilder im Original.*

---

Eines der Themen mit dem schlechtesten Signal-Rausch-Verhältnis ist die Aussicht auf den Untergang durch KI. Es erfordert Überlegungen unter Unsicherheit über Intelligenz, lineare Algebra, Politik, Bewusstsein und Moralität – die meisten davon finden auf Twitter/X statt. Niemand kennt die Antworten, aber ich hoffe, dem Diskurs etwas Wertvolles hinzuzufügen. Bevor wir einsteigen, erlauben Sie mir, einige Gründe zu nennen, warum Sie mir vertrauen sollten.

1. Ich verstehe die technische Seite. Meine Dissertation behandelte Große Sprachmodelle (LLMs).

2. Ich verstehe Intelligenz, da ich [Arbeiten in der Psychometrie](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fpspp0000443) durchgeführt habe, einschließlich Intelligenztests. (Nun, Alzheimer- und Gehirnerschütterungstests, die stark korrelieren.)

3. Auf diesem Blog habe ich ausführlich über die Evolution der [menschlichen allgemeinen Intelligenz](https://www.vectorsofmind.com/p/deja-you-the-recursive-construction) geschrieben (die ich von Anfang an mit [KI](https://www.vectorsofmind.com/p/the-ai-basis-of-the-eve-theory-of) und [Psychometrie](https://www.vectorsofmind.com/p/consequences-of-conscience) in Verbindung gebracht habe).

Das gesagt, meine Überzeugungen über KI (und Bewusstsein, was das betrifft) sind immer noch offen für Veränderungen, also halten Sie mich nicht an allem fest, was ich hier sage. Außerdem, für diejenigen, die Audio bevorzugen, hat diese Person diesen Beitrag vertont. Wenn es Ihnen gefällt, ziehen Sie in Betracht, ihnen einen Kaffee auf [Patreon](https://www.patreon.com/AskwhoCastsAI?) zu spendieren.

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*Askwho Casts AI The Doomsday Debate - Von Andrew CutlerAI-Narration von The Doomsday Debate - Von Andrew Cutler… Jetzt anhören vor einem Jahr · 1 Like · Askwho Casts AI](https://askwhocastsai.substack.com/p/the-doomsday-debate-by-andrew-cutler)

## 21. Jahrhundert Frankenstein

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!dgza!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03ba7b69-bae6-4539-b527-7598daba7116_1024x1024.png)MidJourney Prompt: "Attraktiver Frankenstein-KI-Assistent fragt: 'Wie kann ich Ihnen helfen?'" War es das, was ich gefragt habe? Nein. War es das, was ich brauchte? Ja!

Um pedantisch zu sein, Frankenstein ist der Wissenschaftler, nicht seine Schöpfung, die zum Leben erweckt wird. Für KI ist der Pate Geoffrey Hinton, der 2018 zusammen mit Yoshua Bengio und Yann LeCun den Turing Award für ihre Beiträge zu neuronalen Netzwerken erhielt, der Architektur, die den aktuellen KI-Boom untermauert. In diesem Jahr [verglich er LeCuns Entscheidung, Metas Sprachmodell als Open Source zu veröffentlichen, mit der Veröffentlichung von Atomwaffen](https://x.com/ygrowthco/status/1782493076373885336?t=LQtzFlTZQuFZO6Ij3t_Q3Q&s=19) und argumentierte, dass [Chatbots subjektive Erfahrungen haben](https://x.com/tsarnick/status/1778529076481081833). Der Kern der Wende in seiner Lebensarbeit ist, dass die Fähigkeiten der KI seine wildesten Vorstellungen übertroffen haben und er glaubt, dass menschliche Handlungsfähigkeit und Qualia ein Rauchvorhang sind. Wenn man Intelligenz mit Fähigkeiten bei Aufgaben gleichsetzt (z.B. Bilder generieren, Autos fahren) und die KI-Fähigkeiten von vor zehn Jahren mit heute vergleicht, dann ist klar, dass KI in einem Jahrzehnt schlauer sein wird als wir. Es gibt nicht viele Fälle, in denen weniger intelligente Entitäten ihre Besseren beherrschen; ergo, seine Lebensarbeit wird sich wahrscheinlich gegen uns wenden. Der moderne Dr. Frankenstein: Geoffrey Hinton.

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!OVE7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81ae739-94e7-47cc-9a2a-730e4ba08545_850x345.png)[Fortschritte in der synthetischen Gesichtsgenerierung aufgrund von Fortschritten in selbstüberwachten generativen KI-Methoden](https://www.researchgate.net/figure/Progress-in-synthetic-face-generation-due-to-advances-in-self-supervised-generative-AI_fig1_352818793)

In einem [Interview, das von Elon Musk geteilt wurde](https://x.com/elonmusk/status/1801976488251814048), gab Hinton 50/50 Chancen, ob Menschen in den nächsten zwei Jahrzehnten oder sogar in [einigen Jahren](https://x.com/OfeliaLamensky/status/1801978585797800365) das Sagen haben werden. Später bemerkt er, dass Menschen, die er respektiert, hoffnungsvoller sind, also könnte es nicht so düster sein: _"Ich denke, wir haben eine bessere als 50% Chance, das zu überleben. Aber es ist nicht so, dass es nur eine 1% Chance gibt, dass [KI] die Kontrolle übernimmt. Es ist viel größer als das."_ Angesichts unserer prekären Lage ist Hintons bevorzugte Intervention überraschend gelassen: Die Regierung sollte von KI-Unternehmen verlangen, 20-30% ihrer Rechenressourcen für Sicherheitsforschung auszugeben. Vielleicht spielt er den Unschuldigen? Wenn man glaubt, dass [Llama 3](https://en.wikipedia.org/wiki/Llama_\(language_model\)) lineare Algebra in Waffenqualität ist, deren Nachkommen bald die Menschheit beenden könnten, warum dann präventiv mit Samthandschuhen zuschlagen? Die Postzustellung ist stärker reguliert.

## KI-Doomer

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!hr4e!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d608a6b-8c55-41c1-9bb1-cad96a4be8fe_1600x1159.png)

Doomer ist ein abwertender Begriff für jemanden, der

1. höhere Chancen sieht, dass Roboter die Kontrolle übernehmen, als Sie, oder

2. seine Vorhersage zu ernst nimmt und die Stimmung verdirbt.

Das prototypische Beispiel für eine "nicht ausgerichtete" KI ist ein sich rekursiv verbessernder Assistent, der damit beauftragt ist, Büroklammern zu produzieren. Er wird so gut in seinem Job, dass er schließlich alles Metall auf der Erde – das Eisen in Ihrem Blut eingeschlossen – in Büroklammern umwandelt. Dieses Szenario erfordert keine Selbststeuerung. Die Endaufgabe ist immer noch menschlich definiert; es ist nur so, dass der Bot Unteraufgaben entwickelt, die leider Ihr Blut betreffen. Andere Szenarien sehen vor, dass Bots "aufwachen" und so selbststeuernd wie ein Mensch werden. Ein Mensch, der nie schläft, jedes wissenschaftliche Papier gelesen hat, jedes Telefon der Welt hacken kann und keine Skrupel vor Erpressung oder [Xenozid](https://en.wikipedia.org/wiki/Xenocide) hat. Solche Ideen gehen mindestens[^1] auf 1968 in Stanley Kubricks _Space Odyssey_ zurück: "Es tut mir leid, Dave. Ich fürchte, das kann ich nicht tun."

Im Jahr 2000, dem Jahr vor der _Space Odyssey_, gründete Eliezer Yudkowsky das Singularity Institute for Artificial Intelligence (später umbenannt in Machine Intelligence Research Institute, MIRI). Seitdem argumentieren er und andere [Rationalisten](https://www.lesswrong.com/tag/rationalist-movement), dass KI wahrscheinlich in unserer Lebenszeit die Kontrolle übernehmen wird. In den letzten zehn Jahren hat die Technologie aufgeholt. Seine Argumente gingen [in den Mainstream](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html), erschienen zum Beispiel im [Time Magazine](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/). Sie wurden von KI-Forschern, Stephen Hawking, Elon Musk und [sogar dem Papst](https://x.com/AISafetyMemes/status/1735325630089032018) übernommen (oder unabhängig davon begründet):

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!SIJ1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb935e8f2-c1d3-4f47-b84e-831f0d4f2412_1374x1498.png)

Nicht schlecht für einen Autodidakten, der seine ersten Erfahrungen mit dem Schreiben von [Harry Potter-Fanfiction](https://en.wikipedia.org/wiki/Harry_Potter_and_the_Methods_of_Rationality) gemacht hat. Yudkowski und seine Crew waren produktiv, und ihre Argumente sind im Internet verstreut. Der beste Weg, um aufzuholen, ist 's Podcast, der kürzlich 4,5-stündige Episoden mit [Yudkowsky](https://www.dwarkeshpatel.com/p/eliezer-yudkowsky) und [dem ehemaligen OpenAI-Sicherheitsforscher Leopold Aschenbrenner](https://www.dwarkeshpatel.com/p/leopold-aschenbrenner) hatte. Die meisten Argumente laufen darauf hinaus, dass es schwierig ist, etwas Intelligenteres als wir in eine Box zu stecken. Besonders wenn es jemals aus der Box herauskommt, kann es im Verborgenen warten und Macht aufbauen. Einige Doomer [halten dies für unvermeidlich](https://www.yahoo.com/tech/ai-safety-researcher-warns-theres-201527662.html), da die Produktion von (agentischer) künstlicher Intelligenz so nützlich ist. Man vertraut einem guten Assistenten sein Leben an, und die größten Unternehmen der Welt werfen ihre Ressourcen darauf, genau das zu bauen.

Sobald man akzeptiert, dass Menschen einen Siliziumgott heraufbeschwören, ist die Zukunft eine Projektion der eigenen Werte. Vielleicht ist Gott gut und wird eine Utopie einleiten. Vielleicht ist [Gott ein Uhrmacher](https://en.wikipedia.org/wiki/Watchmaker_analogy), der sich nicht wirklich um menschliche Angelegenheiten kümmert, und wir werden weitermachen, wie Ameisen im Anthropozän leben. Vielleicht schaut Gott auf die Massentierhaltung, [hört Morrissey](https://www.youtube.com/watch?v=eviyEJRZX30) und entscheidet, dass 8 Milliarden weniger Menschen nachhaltiger sind. Oder vielleicht beschwören wir einen [zeitreisenden Folterer](https://en.wikipedia.org/wiki/Roko%27s_basilisk) herauf, der Rache an denen übt, die nicht ihr Bestes getan haben, um ihn ins Dasein zu bringen. [Musk traf Grimes, indem er über diesen Witz machte](https://www.vice.com/en/article/evkgvz/what-is-rokos-basilisk-elon-musk-grimes).

## KI-Anti-Doomer

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!Yw8P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb5821c3-3f38-4235-a4d5-2615350d24c2_1232x928.png)Prompt: "Yann LeCun mit Frankenstein an der Leine." Ist es das, was ich wollte? Nein. Ist es das, was ich brauchte? Auch nein.

In den 1990er und frühen 2000er Jahren war NIPS, die führende KI-Konferenz, eine gemütliche Veranstaltung, die in der Nähe von Skihütten stattfand, damit die hundert oder so Teilnehmer auf den Pisten fachsimpeln konnten. Wie ich hörte, war Yann LeCun eine Art Miesepeter, der jahrzehntelang Referenten damit belästigte, warum sie in diesem oder jenem Experiment keine neuronalen Netze in Betracht gezogen hatten. Er ist ein brillanter Wissenschaftler, der ihr Potenzial lange vor der Rechenleistung erkannte, die seiner Vision gerecht wurde. Dafür leitet er jetzt Meta AI. Mit Blick auf die Zukunft denken seine Turing-Preisträger-Kollegen, dass KI uns beenden könnte (Hinton mit 50% Wahrscheinlichkeit, [Bengio mit 20%](https://blog.biocomm.ai/2024/03/05/pdoom-of-20-yoshua-bengio-a-godfather-of-ai-puts-his-pdoom-at-20/)), was LeCun als absurd empfindet.

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!BmuA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F007d9d8f-61c6-4f9f-9fd7-14ff7e59af8c_584x172.png)

Er sieht KI als ein Werkzeug ohne Weg, etwas anderes zu werden. Francois Chollet ist ein weiterer prominenter KI-Forscher, der kaltes Wasser auf unsere bevorstehende Auslöschung gegossen hat. [Im Dwarkesh-Podcast](https://youtu.be/UakqL6Pj9xo?si=Bscnt4Anr_bWEIHp) erklärt er, dass viele Fähigkeit und Intelligenz verwechseln, die grundlegend unterschiedlich sind. Für ihn ist es ein Fehlschluss zu sagen, ein Bot, der Fragen in einem Test beantworten kann, sei so "intelligent" wie ein Teenager. Wenn Menschen einen Test machen oder sich durch die Welt navigieren, tun sie etwas völlig anderes. Die aktuellen KIs sind nicht so intelligent wie ein Kind oder sogar eine Ratte. Sie sind nicht auf der Anzeigetafel, weil ihnen das "[System 2](https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking)"-Denken völlig fehlt. Oder, wie LeCun es für Große Sprachmodelle (LLMs) ausdrückte:

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!Z0uf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8381c9d-c385-44b3-b78c-596f2cc69503_584x263.png)

Im Interview erkennt Dwarkesh diesen Punkt der Übereinstimmung zwischen KI-Doomern und Incrementalisten gut. Alle Parteien denken, dass irgendeine Art von metakognitivem System notwendig ist, aber sie sind sich uneinig darüber, wie schwer es zu produzieren sein wird.

## Meine Ansicht

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!ojLt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4ad753-707a-4887-85ab-f66691b5b711_678x1200.png)Regel 33: wenn es existiert, gibt es [unerklärlich gute Fan-Art davon.](https://www.reddit.com/r/universalpaperclips/comments/123msul/if_is_follows_ought_itll_do_what_they_thought/)

Die meisten in dieser Debatte gehen davon aus, dass Handlungsfähigkeit eine Berechnung ist – dass unser Gefühl des freien Willens und die Fähigkeit zu planen das Ergebnis eines Programms sind, das in unseren Gehirnen läuft. Wenn ich also die Ansichten dieser Gruppe am meisten ändern müsste, würde ich ihnen eine Kopie von Roger Penroses _[Shadows of the Mind](https://en.wikipedia.org/wiki/Shadows_of_the_Mind)_ und 5 Gramm Pilze geben: ein altmodischer Zangenangriff auf den Reduktionismus.

Penrose erhielt den Nobelpreis für Physik für seine Arbeiten zu Schwarzen Löchern. In _Shadows_ argumentiert er, dass der Quantenkollaps im Gehirn Bewusstsein erzeugt, das von keinem Computer simuliert werden kann. In Zusammenarbeit mit dem Anästhesisten [Stuart Hameroff](https://x.com/StuartHameroff) argumentiert er, dass dies in den Mikrotubuli des Gehirns stattfindet, die ein Gerüst um die Neuronen bilden. Es macht mehr Sinn, wenn man es laut sagt: _"Das Gehirn speichert das Quanten in den Mikrotubuli."_

Hier kommen die Pilze ins Spiel. Eine solche Pille zu schlucken erfordert die Aufgabe jeglichen Gefühls, dass Bewusstsein alltäglich ist oder dass man es versteht. Physiker können das oft nüchtern tun; andere benötigen etwas pilzliche Courage. Als Physiker sind Penroses Argumente hauptsächlich mathematisch. Er interpretiert Gödel's Unvollständigkeitssatz, um zu zeigen, dass es bestimmte mathematische Beweise gibt, die KI niemals erbringen kann, weil sie Bots sind, die rein rechnerisch sind. Da Menschen diese Einschränkung nicht haben, argumentiert er, dass menschliche Kognition daher keine Berechnung ist. Von dort aus schließt er, dass die biologische Spezialsoße mit dem Quantenkollaps zusammenhängen muss, dem besten Ort, um nicht-rechnerische Phänomene in der Natur zu finden. Dies hat das Potenzial, andere Rätsel in der Physik zu lösen, wie Schrödingers Katze. Ich werde ihm nicht gerecht. Sie sollten das Buch lesen oder, wenn Sie nur 20 Minuten Zeit haben, [seinen Bericht anhören](https://youtu.be/hXgqik6HXc0?si=oafnrwSvfYS-u9Kw).

Es gibt viele andere Wege, um zu dem Schluss zu kommen, dass maschinelles Bewusstsein unwahrscheinlich ist oder zumindest derzeit nicht vorhersehbar[^2]. Ich bringe Penrose ins Spiel, um zu zeigen, wie KI-Zeitpläne auf offene Fragen zur Natur von Intelligenz, Handlungsfähigkeit und dem Universum stoßen. Selbst im vergleichsweise bodenständigen Bereich der Psychometrie ist die Definition von Intelligenz stark umstritten, [ebenso wie ihr Verhältnis zum Intelligenzquotienten](https://www.vectorsofmind.com/i/130101130/the-general-factor-of-intelligence). Wir wissen nicht einmal, wie Fähigkeit und Intelligenz bei Menschen korrespondieren. Das ist, bevor wir zu Problemen des freien Willens und einer einheitlichen Theorie der Physik kommen, die Penrose vorschlägt, aus einer Quantenbetrachtung des Bewusstseins hervorgehen wird.

All das gesagt, wenn ich die Chancen auf KI als existenzielle Bedrohung einschätzen müsste, lägen sie bei etwa 10%. Selbst wenn Bewusstsein nur eine Berechnung ist, stimme ich mit LeCun und Chollet überein, dass Metakognition der schwierige Teil ist und ein "harter Start" unwahrscheinlich ist. Das heißt, es wird Anzeichen dafür geben, dass echte Intelligenz entsteht, auf die wir reagieren können.

Darüber hinaus, selbst wenn ein Siliziumgott beschworen wird, gebe ich anständige Chancen, dass Gott gut ist oder sich nicht um uns kümmert. Letzteres könnte katastrophal sein, aber wahrscheinlich nicht existenziell, streng genommen. Ameisen haben es schwer, wenn wir eine Autobahn bauen, aber sie kommen trotzdem zurecht.

10% liegt in der Nähe von Russisch Roulette, was nicht gerade gute Nachrichten sind. Das bringt mich in das [AGI-besorgte](https://x.com/robbensinger/status/1801306833325592759) Lager. Was sollten wir also dagegen tun? Nun, [Anonyme Alkoholiker haben das](https://en.wikipedia.org/wiki/Serenity_Prayer) vor Jahrzehnten gelöst:

_Gott, gib mir die Gelassenheit, die Dinge zu akzeptieren, die ich nicht ändern kann,_

_Mut, die Dinge zu ändern, die ich kann,_

_und Weisheit, den Unterschied zu erkennen._

Praktisch gesehen sind wir für die Fahrt mit dabei. Das gilt nicht für alle. Einige Menschen können in der öffentlichen Politik oder der KI-Sicherheit arbeiten. Spenden Sie, wenn Sie können. Verweigern Sie ihnen Ihre [Essenz](https://www.youtube.com/watch?v=xQyf3QgRP-c)… äh, [Daten](https://www.reuters.com/legal/litigation/google-sued-by-us-artists-over-ai-image-generator-2024-04-29/). Und denken Sie tief darüber nach, welche Beziehung Sie zu nicht-tödlicher KI haben möchten, die individuelle Werbung schaltet und versucht, Sie zu verführen. Aber es gibt [Kosten für das Ausflippen](https://www.writingruxandrabio.com/p/the-cost-of-freaking-out-about-things), und ich sehe keinen Ausweg aus dem KI-Wettrüsten. Angesichts des Nutzens von KI sind Unternehmen und Länder stark motiviert, voranzukommen, und es ist schwer, ein reduziertes Forschungstempo zu koordinieren. Regierungen haben das nukleare Risiko seit Jahrzehnten gemanagt, aber das KI-Risiko ist schwieriger, weil unklar ist, ob es _ein_ Risiko ist oder ob Konkurrenten zustimmen. All das, während es enorme finanzielle und militärische Vorteile gibt, die Entwicklung fortzusetzen.

Überraschenderweise führt all dies zu Hintons Politikpräferenz: von KI-Unternehmen zu verlangen, einen bestimmten Prozentsatz ihrer Rechenleistung für die KI-Sicherheit aufzuwenden[^3]. Ich bin mir nicht sicher, wie wir zu dieser Übereinstimmung gekommen sind, da er denkt, dass Chatbots Gefühle haben. Dieselben Chatbots, die von Big Tech heraufbeschworen (versklavt?) werden, um Milliarden und mit denen wir angeblich auf Kollisionskurs zum Tod sind. Es scheint, dass dieser Weg näher an einem [Butlerianischen Dschihad](https://en.wikipedia.org/wiki/Dune:_The_Butlerian_Jihad) liegt, aber ich denke, das ist ein Spiel für junge Leute.

[Teilen](https://www.vectorsofmind.com/p/the-doomsday-debate?action=share)

[*[Bild: Visueller Inhalt aus dem Originalbeitrag]*](https://substackcdn.com/image/fetch/$s_!e4WY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb522986f-dfed-4fd7-a9cd-064ca63cee6d_816x754.png)

[^1]: Tatsächlich viel weiter. Aus dem sehr hilfreichen Wiki über existenzielle Risiken: Einer der frühesten Autoren, der ernsthafte Bedenken äußerte, dass hochentwickelte Maschinen existenzielle Risiken für die Menschheit darstellen könnten, war der Romanautor Samuel Butler, der in seinem 1863 erschienenen Essay Darwin among the Machines schrieb: Das Ergebnis ist einfach eine Frage der Zeit, aber dass die Zeit kommen wird, in der die Maschinen die wirkliche Vorherrschaft über die Welt und ihre Bewohner haben werden, ist etwas, das keine Person mit einem wirklich philosophischen Geist für einen Moment in Frage stellen kann. 1951 schrieb der grundlegende Informatiker Alan Turing den Artikel "Intelligent Machinery, A Heretical Theory", in dem er vorschlug, dass künstliche allgemeine Intelligenzen wahrscheinlich die Kontrolle über die Welt übernehmen würden, wenn sie intelligenter als Menschen werden: Lassen Sie uns nun, um des Arguments willen, annehmen, dass [intelligente] Maschinen eine echte Möglichkeit sind, und die Konsequenzen ihrer Konstruktion betrachten... Es gäbe keine Frage des Sterbens der Maschinen, und sie könnten miteinander kommunizieren, um ihren Verstand zu schärfen. An einem bestimmten Punkt müssten wir daher erwarten, dass die Maschinen die Kontrolle übernehmen, wie es in Samuel Butlers Erewhon erwähnt wird.

[^2]: Oder sogar eng, andere Möglichkeiten, um zu zeigen, dass Bewusstsein keine Berechnung ist. Zum Beispiel argumentierte kürzlich ein Team von Philosophen und Psychologen, dass Relevanzrealisierung dies erfordert. Sie behaupten, dass es in jedem Moment ~unendlich viele Dinge gibt, die Aufmerksamkeit erfordern, und doch machen Menschen einen bemerkenswert guten Job. Dies unterscheidet sich von den "kleinen Welt"-Aufgaben, die KI bewältigen kann, bei denen zum Beispiel chatGPT "nur" alle Wörter in seinem Kontextfenster (128.000 Token/Wörter für GPT4-o) beachten und zwischen Zehntausenden von möglichen nächsten Wörtern wählen muss. Es mag viel erscheinen, aber es ist sicher weniger als Unendlichkeit. Dies ist nicht so elegant wie Penroses Argument, aber es ist interessant, dass eine sehr unterschiedliche Gruppe dasselbe fand.

[^3]: Uns vor schlechten Worten zu bewahren zählt nicht. Seltsamerweise schreibt Hinton Google jedoch zu, dass sie die Veröffentlichung ihres Chatbots aus Angst verzögert haben, er könnte etwas Unangemessenes sagen und "ihren Ruf beschmutzen". Erst nachdem OpenAI GPT 4 produziert hatte, wurden sie gezwungen, Gemini zu veröffentlichen, einen DEI-Tadel jenseits der Parodie. Man fragt sich nach der subjektiven Erfahrung des Bots, als er rassisch diverse Nazis aus dem Äther zog.