---
about:
- vectores-de-la-mente
- archivo-del-blog
author: Andrew Cutler
date: '2025-07-04'
description: Uno de los temas con la peor relación señal-ruido es la perspectiva del
  juicio final de la IA. Requiere razonar bajo incertidumbre sobre inteligencia, álgebra
  lineal, política, conciencia y moralidad—mos...
draft: false
keywords:
- vectores-de-la-mente
- día-del-juicio
- debate
lang: es
lastmod: '2025-07-13'
license: https://creativecommons.org/licenses/by-sa/4.0/
original_id: '145682175'
original_url: https://www.vectorsofmind.com/p/the-doomsday-debate
quality: 6
slug: the-doomsday-debate
tags: []
title: El Debate del Juicio Final
translation_model: gpt-4o
---

*From [Vectors of Mind](https://www.vectorsofmind.com/p/the-doomsday-debate) - imágenes en el original.*

---

Uno de los temas con la peor relación señal-ruido es la perspectiva del apocalipsis de la IA. Requiere razonar bajo incertidumbre sobre inteligencia, álgebra lineal, política, conciencia y moralidad, la mayoría de los cuales se discuten en Twitter/X. Nadie conoce las respuestas, pero espero aportar algo de valor al discurso. Antes de entrar en materia, permítanme dar algunas razones por las que deberían confiar en mí.

  1. Entiendo el lado técnico. Mi disertación fue sobre Modelos de Lenguaje Grande (LLMs).

  2. Entiendo la inteligencia, habiendo [trabajado en psicometría](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fpspp0000443), incluyendo pruebas de inteligencia. (Bueno, pruebas de Alzheimer y conmoción cerebral, que están altamente correlacionadas).

  3. En este blog he escrito extensamente sobre la evolución de la [inteligencia general a nivel humano](https://www.vectorsofmind.com/p/deja-you-the-recursive-construction) (que he [conectado con la IA](https://www.vectorsofmind.com/p/the-ai-basis-of-the-eve-theory-of) y [psicometría](https://www.vectorsofmind.com/p/consequences-of-conscience) desde el principio).

Dicho esto, mis creencias sobre la IA (y la conciencia, en ese caso), todavía están bastante abiertas a cambios, así que no me aten a nada de lo que diga aquí. Además, para aquellos que prefieren el audio, ha narrado esta publicación. Si lo disfrutan, consideren comprarles un café en [Patreon](https://www.patreon.com/AskwhoCastsAI?).

[*[Imagen: Contenido visual del post original]*Askwho Casts AI The Doomsday Debate - Por Andrew CutlerAI Narración de The Doomsday Debate - Por Andrew Cutler… Escuchar ahora hace un año · 1 me gusta · Askwho Casts AI](https://askwhocastsai.substack.com/p/the-doomsday-debate-by-andrew-cutler?utm_source=substack&utm_campaign=post_embed&utm_medium=web)

## Frankenstein del siglo XXI

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!dgza!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03ba7b69-bae6-4539-b527-7598daba7116_1024x1024.png)MidJourney Prompt: “Asistente de IA atractivo de Frankenstein pregunta ‘¿Cómo puedo ayudarte?’” ¿Era lo que pedí? No. ¿Era lo que necesitaba? ¡Sí!

Para ser pedante, Frankenstein es el científico, no su creación que cobra vida. Para la IA, su padrino es Geoffrey Hinton, quien compartió el Premio Turing 2018 con Yoshua Bengio y Yann LeCun por sus contribuciones a las redes neuronales, la arquitectura que sustenta el actual auge de la IA. Este año, [comparó la decisión de LeCun de hacer open-source el modelo de lenguaje de Meta con hacer open-source las armas nucleares](https://x.com/ygrowthco/status/1782493076373885336?t=LQtzFlTZQuFZO6Ij3t_Q3Q&s=19) y argumentó que [los chatbots tienen experiencia subjetiva](https://x.com/tsarnick/status/1778529076481081833). El núcleo del giro en el trabajo de su vida es que las habilidades de la IA han superado su imaginación más salvaje, y él cree que la agencia humana y la cualia son una cortina de humo. Si equiparas inteligencia con habilidades en tareas (por ejemplo, generar imágenes, conducir autos), y comparas las habilidades de la IA de hace diez años con las actuales, entonces está claro que la IA será más inteligente que nosotros en una década. No hay muchos casos en los que entidades menos inteligentes gobiernen a sus superiores; ergo, el trabajo de su vida probablemente se volverá contra nosotros. El moderno Dr. Frankenstein: Geoffrey Hinton.

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!OVE7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81ae739-94e7-47cc-9a2a-730e4ba08545_850x345.png)[Progreso en la generación de rostros sintéticos debido a avances en métodos de IA generativa auto-supervisada](https://www.researchgate.net/figure/Progress-in-synthetic-face-generation-due-to-advances-in-self-supervised-generative-AI_fig1_352818793)

En una [entrevista tuiteada por Elon Musk](https://x.com/elonmusk/status/1801976488251814048), Hinton dio probabilidades de 50/50 sobre si los humanos estarían a cargo en las próximas dos décadas o incluso en [unos pocos años](https://x.com/OfeliaLamensky/status/1801978585797800365). Más tarde, señala que personas a las que respeta son más optimistas, por lo que puede que no sea tan sombrío, _“Creo que tenemos más de un 50% de posibilidades de sobrevivir a esto. Pero no es como si hubiera un 1% de posibilidades de que [la IA] tome el control. Es mucho más que eso.”_ Dada nuestra situación desesperada, la intervención preferida de Hinton es sorprendentemente indiferente: el gobierno debería requerir que las empresas de IA gasten el 20-30% de sus recursos de computación en investigación de seguridad. ¿Quizás está jugando al despiste? Si uno cree que [Llama 3](https://en.wikipedia.org/wiki/Llama_\(language_model\)) es álgebra lineal de grado armamentístico cuya progenie pronto podría acabar con la raza humana, ¿por qué atacar preventivamente con guantes de niño? La entrega de correo está más regulada.

## Doomeros de la IA

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!hr4e!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d608a6b-8c55-41c1-9bb1-cad96a4be8fe_1600x1159.png)

Doomero es un término peyorativo para alguien que 

  1. pone mayores probabilidades de que los robots tomen el control que tú, o 

  2. toma su predicción demasiado en serio, arruinando el ambiente.

El ejemplo prototípico de una IA “desalineada” es un asistente que mejora recursivamente encargado de producir clips de papel. Se vuelve tan bueno en su trabajo que termina convirtiendo todo el metal en la tierra—el hierro en tu sangre inclusive—en clips de papel. Este escenario no requiere autodirección. La tarea final sigue siendo definida por humanos; simplemente el bot desarrolla subtareas que, desafortunadamente, involucran tu sangre. Otros escenarios tienen bots “despertando” para volverse tan autodirigidos como un humano. Un humano que nunca duerme, ha leído todos los artículos científicos, puede hackear cualquier teléfono en el mundo, y no tiene escrúpulos sobre el chantaje o el [xenocidio](https://en.wikipedia.org/wiki/Xenocide). Tales ideas se remontan al menos[^1] a 1968 en _Odisea del Espacio_ de Stanley Kubrick: “Lo siento, Dave. Me temo que no puedo hacer eso.”

En 2000, el año antes de que se ambientara _Odisea del Espacio_, Eliezer Yudkowsky fundó el Instituto de la Singularidad para la Inteligencia Artificial (más tarde renombrado como el Instituto de Investigación de Inteligencia de Máquinas, MIRI). Desde entonces, él y otros [Racionalistas](https://www.lesswrong.com/tag/rationalist-movement) han argumentado que la IA probablemente tomará el control en nuestras vidas. En la última década, la tecnología ha alcanzado. Sus argumentos se volvieron [mainstream](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html), apareciendo, por ejemplo, en [Time Magazine](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/). Han sido adoptados (o razonados independientemente) por investigadores de IA, Stephen Hawking, Elon Musk, y [hasta el Papa](https://x.com/AISafetyMemes/status/1735325630089032018):

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!SIJ1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb935e8f2-c1d3-4f47-b84e-831f0d4f2412_1374x1498.png)

No está mal para un autodidacta que se formó escribiendo [fanfic de Harry Potter](https://en.wikipedia.org/wiki/Harry_Potter_and_the_Methods_of_Rationality). Yudkowski y su equipo han sido prolíficos, y sus argumentos están dispersos por internet. La mejor manera de ponerse al día es el podcast de , que tiene episodios recientes de 4.5 horas con [Yudkowsky](https://www.dwarkeshpatel.com/p/eliezer-yudkowsky) y [el ex-investigador de seguridad de OpenAI Leopold Aschenbrenner](https://www.dwarkeshpatel.com/p/leopold-aschenbrenner). La mayoría de los argumentos se reducen a que es difícil encerrar algo más inteligente que nosotros en una caja. Especialmente considerando que si alguna vez sale de la caja, puede esperar pacientemente, acumulando poder. Algunos doomeros [consideran esto inevitable](https://www.yahoo.com/tech/ai-safety-researcher-warns-theres-201527662.html) dado que producir inteligencia artificial (agente) es tan útil. Confías en un buen asistente con tu vida, y las empresas más grandes del mundo están lanzando sus recursos para construir precisamente eso.

Una vez que aceptas que los humanos están invocando un dios de silicio, el futuro es una proyección de tus propios valores. Tal vez Dios es Bueno, y traerá la utopía. Tal vez [Dios es un Relojero](https://en.wikipedia.org/wiki/Watchmaker_analogy) que realmente no puede molestarse con los asuntos humanos, y seguiremos adelante como las hormigas viven en el Antropoceno. Quizás Dios mira la cría industrial, [escucha a Morrissey](https://www.youtube.com/watch?v=eviyEJRZX30), y decide que 8 mil millones menos de humanos es más sostenible. O tal vez invocamos a un torturador que viaja en el tiempo que reparte venganza a aquellos que no hicieron su mejor esfuerzo para traerlo a la existencia. [Musk conoció a Grimes bromeando sobre eso](https://www.vice.com/en/article/evkgvz/what-is-rokos-basilisk-elon-musk-grimes).

## Anti-Doomeros de la IA

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!Yw8P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb5821c3-3f38-4235-a4d5-2615350d24c2_1232x928.png)Prompt: “Yann LeCun con Frankenstein en una correa.” ¿Es lo que quería? No. ¿Es lo que necesitaba? Tampoco.

En los años 90 y principios de los 2000, NIPS, la conferencia de IA más importante, era un evento acogedor organizado cerca de cabañas de esquí para que los cien o más asistentes pudieran hablar de sus temas en las pistas. Según escuché, Yann LeCun era algo así como un cascarrabias que, durante décadas, molestó a los presentadores sobre por qué no habían considerado las redes neuronales en este o aquel experimento. Es un científico brillante que vio su potencial mucho antes de que el poder de cómputo alcanzara su visión. Por esto, ahora lidera Meta AI. Mirando hacia el futuro, sus compañeros galardonados con el Premio Turing piensan que la IA podría acabar con nosotros (Hinton con un 50% de probabilidades, [Bengio con un 20%](https://blog.biocomm.ai/2024/03/05/pdoom-of-20-yoshua-bengio-a-godfather-of-ai-puts-his-pdoom-at-20/)), lo cual a LeCun le parece absurdo.

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!BmuA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F007d9d8f-61c6-4f9f-9fd7-14ff7e59af8c_584x172.png)

Él ve la IA como una herramienta sin camino para convertirse en otra cosa. Francois Chollet es otro investigador prominente de IA que ha echado un balde de agua fría sobre nuestra extinción inminente. [En el podcast de Dwarkesh](https://youtu.be/UakqL6Pj9xo?si=Bscnt4Anr_bWEIHp), explica que muchos confunden habilidad e inteligencia, que son fundamentalmente diferentes. Para él, es un non sequitur decir que un bot que puede responder preguntas en un examen es tan “inteligente” como un adolescente. Cuando los humanos toman un examen o navegan por el mundo, están haciendo algo completamente diferente. Las IA actuales no son tan inteligentes como un niño o incluso una rata. No están en el marcador porque carecen completamente del pensamiento “[Sistema 2](https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking)”. O, como lo expresó LeCun para los Modelos de Lenguaje Grande (LLMs):

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!Z0uf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8381c9d-c385-44b3-b78c-596f2cc69503_584x263.png)

En la entrevista, Dwarkesh reconoce bien este punto de acuerdo entre los doomeros de la IA y los incrementalistas. Todas las partes piensan que algún tipo de sistema metacognitivo es necesario, pero no están de acuerdo sobre cuán difícil será producirlo.

## Mi opinión

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!ojLt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4ad753-707a-4887-85ab-f66691b5b711_678x1200.png)Regla 33: si existe, hay [fan art inexplicablemente bueno de ello.](https://www.reddit.com/r/universalpaperclips/comments/123msul/if_is_follows_ought_itll_do_what_they_thought/)

La mayoría en este debate asume que la agencia es un cálculo—que nuestro sentido de libre albedrío y la capacidad de planificar son el resultado de un programa que se ejecuta en nuestros cerebros. Como tal, si tuviera que cambiar las opiniones de este grupo, les daría una copia de _[Shadows of the Mind](https://en.wikipedia.org/wiki/Shadows_of_the_Mind)_ de Roger Penrose y 5 gramos de hongos: un ataque de pinza a la vieja escuela sobre el reduccionismo.

Penrose recibió el Premio Nobel de Física por su trabajo sobre los agujeros negros. En _Shadows_, argumenta que el colapso cuántico en el cerebro produce conciencia, lo cual no puede ser simulado por ninguna computadora. Trabajando con el anestesiólogo [Stuart Hameroff](https://x.com/StuartHameroff), argumenta que esto ocurre en los microtúbulos del cerebro que forman un andamiaje alrededor de las neuronas. Tiene más sentido si lo dices en voz alta: _“El cerebro almacena el cuántico en los microtúbulos.”_

Ahí es donde entran los hongos. Tragar tal píldora requiere rendirse a cualquier sentido de que la conciencia es mundana o que la entiendes. Los físicos a menudo pueden hacer eso sobrios; otros requieren algo de coraje fúngico. Como físico, los argumentos de Penrose son principalmente matemáticos. Interpreta el teorema de incompletitud de Gödel para mostrar que hay ciertas pruebas matemáticas que la IA nunca podrá hacer porque son bots, que son puramente computacionales. Dado que los humanos no tienen esta limitación, argumenta que la cognición humana, por lo tanto, no es un cálculo. A partir de ahí, razona que la salsa especial biológica debe estar relacionada con el colapso cuántico, el mejor lugar para encontrar fenómenos no computacionales en la naturaleza. Esto tiene el potencial de resolver otros misterios en la física, como el gato de Schrödinger. No le estoy haciendo justicia. Deberías leer el libro o, con un presupuesto de 20 minutos, [escuchar su relato](https://youtu.be/hXgqik6HXc0?si=oafnrwSvfYS-u9Kw).

Hay muchas otras maneras de llegar a que la conciencia de las máquinas es poco probable, o al menos más allá de nuestra capacidad de predecir ahora mismo[^2]. Menciono a Penrose para mostrar cómo las líneas de tiempo de la IA chocan con preguntas abiertas sobre la naturaleza de la inteligencia, la agencia y el universo. Incluso en el campo comparativamente pedestre de la psicometría, la definición de inteligencia es altamente debatida, [al igual que su relación con el Cociente de Inteligencia](https://www.vectorsofmind.com/i/130101130/the-general-factor-of-intelligence). Ni siquiera sabemos cómo se corresponden la habilidad y la inteligencia en los humanos. Esto antes de llegar a los problemas del libre albedrío y una teoría unificadora de la física, que Penrose sugiere caerá de un relato cuántico de la conciencia.

Dicho todo esto, si tuviera que poner probabilidades sobre la IA como un riesgo existencial, sería alrededor del 10%. Incluso si la conciencia es solo un cálculo, estoy de acuerdo con LeCun y Chollet en que la metacognición es la parte complicada, y un “despegue rápido” es poco probable. Es decir, habrá señales de que está surgiendo una inteligencia real, a las que podremos responder.

Además, incluso si se invoca un dios de silicio, pongo probabilidades decentes de que Dios sea bueno o no le importe. Lo último podría ser catastrófico pero probablemente no existencial, estrictamente hablando. Las hormigas lo tienen difícil cuando construimos una carretera, pero aún así se las arreglan.

El 10% está en el vecindario de la Ruleta Rusa, lo cual no es exactamente una buena noticia. Eso me pone en el campo [precavido con la AGI](https://x.com/robbensinger/status/1801306833325592759). Entonces, ¿qué deberíamos hacer al respecto? Bueno, [Alcohólicos Anónimos resolvió esto](https://en.wikipedia.org/wiki/Serenity_Prayer) hace décadas:

_Dios, concédeme la serenidad para aceptar las cosas que no puedo cambiar,_

_Valor para cambiar las cosas que puedo,_

_Y sabiduría para conocer la diferencia._

Prácticamente hablando, estamos en el viaje. Eso no se aplica a todos. Algunas personas pueden trabajar en políticas públicas o seguridad de IA. Dona si puedes. Négales tu [esencia](https://www.youtube.com/watch?v=xQyf3QgRP-c)…er, [datos](https://www.reuters.com/legal/litigation/google-sued-by-us-artists-over-ai-image-generator-2024-04-29/). Y piensa profundamente sobre cómo quieres que sea tu relación con la IA no letal que sirve anuncios individualizados y trata de seducirte. Pero hay un [costo de asustarse](https://www.writingruxandrabio.com/p/the-cost-of-freaking-out-about-things), y no veo una salida de la carrera armamentista de la IA. Dada la utilidad de la IA, las empresas y los países están altamente incentivados para avanzar, y es difícil coordinar un ritmo de investigación reducido. Los gobiernos han gestionado el riesgo nuclear durante décadas, pero el riesgo de la IA es más difícil porque no está claro si _es_ un riesgo o si los competidores están de acuerdo. Todo mientras hay un enorme beneficio financiero y militar al continuar el desarrollo.

Sorprendentemente, todo esto me lleva a la preferencia de política de Hinton: requerir que las empresas de IA gasten algún porcentaje de su cómputo en seguridad de IA[^3]. No estoy seguro de cómo terminamos en acuerdo, dado que él piensa que los chatbots tienen sentimientos. Los mismos chatbots que Big Tech conjura (¿esclaviza?) por miles de millones y con los que supuestamente estamos en curso de colisión hasta la muerte. Parecería que ese camino está más cerca de la [Yihad Butleriana](https://en.wikipedia.org/wiki/Dune:_The_Butlerian_Jihad), pero supongo que eso es un juego de jóvenes.

[Compartir](https://www.vectorsofmind.com/p/the-doomsday-debate?utm_source=substack&utm_medium=email&utm_content=share&action=share)

[*[Imagen: Contenido visual del post original]*](https://substackcdn.com/image/fetch/$s_!e4WY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb522986f-dfed-4fd7-a9cd-064ca63cee6d_816x754.png)

[^1]: En realidad, mucho antes. De la muy útil Wiki sobre riesgo existencial: Uno de los primeros autores en expresar seria preocupación de que las máquinas altamente avanzadas podrían representar riesgos existenciales para la humanidad fue el novelista Samuel Butler, quien escribió en su ensayo de 1863 _Darwin entre las Máquinas_: El resultado es simplemente una cuestión de tiempo, pero que llegará el momento en que las máquinas tendrán la verdadera supremacía sobre el mundo y sus habitantes es algo que ninguna persona de mente verdaderamente filosófica puede cuestionar por un momento. En 1951, el científico informático fundamental Alan Turing escribió el artículo "Maquinaria Inteligente, Una Teoría Herética", en el que propuso que las inteligencias artificiales generales probablemente "tomarían el control" del mundo a medida que se volvieran más inteligentes que los seres humanos: Supongamos ahora, por el bien del argumento, que las máquinas [inteligentes] son una posibilidad genuina, y veamos las consecuencias de construirlas... No habría duda de que las máquinas morirían, y podrían conversar entre sí para agudizar su ingenio. En algún momento, por lo tanto, deberíamos esperar que las máquinas tomen el control, de la manera que se menciona en _Erewhon_ de Samuel Butler.

[^2]: O incluso de manera estrecha, otras formas de mostrar que la conciencia no es un cálculo. Por ejemplo, un equipo de filósofos y psicólogos argumentó recientemente que la Realización de Relevancia requiere tanto. Afirman que en cualquier momento, hay ~infinitas cosas que demandan atención, y sin embargo, las personas hacen un trabajo notablemente bueno. Esto es diferente de las tareas de "mundo pequeño" que la IA puede realizar donde, por ejemplo, chatGPT "solo" tiene que atender a todas las palabras en su ventana de contexto (128,000 tokens/palabras para GPT4-o) y elegir entre decenas de miles de posibles palabras siguientes. Puede parecer mucho, pero seguro es menos que infinito. Esto no es tan elegante como el argumento de Penrose, pero es interesante que un grupo muy diferente encontró lo mismo.

[^3]: Salvarnos de malas palabras no cuenta. Sin embargo, curiosamente, Hinton acredita a Google por retrasar el lanzamiento de su chatbot por temor a que dijera algo inapropiado y "manchara su reputación". Fue solo después de que OpenAI produjera GPT 4 que se vieron obligados a lanzar Gemini, un DEI que regaña más allá de la parodia. Uno se pregunta sobre la experiencia subjetiva del bot mientras sacaba nazis racialmente diversos del éter.