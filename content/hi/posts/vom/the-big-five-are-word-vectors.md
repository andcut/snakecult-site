---
about:
- मन के वेक्टर
- ब्लॉग-संग्रह
author: Andrew Cutler
date: '2025-07-04'
description: मनोविज्ञान में शब्दार्थ अध्ययन और कंप्यूटर विज्ञान में लेटेंट सेमांटिक
  विश्लेषण को अलग-अलग समस्याओं को हल करने के लिए आधी सदी के अंतराल पर पेश किया गया
  था और फिर भी वे गणितीय रूप से समकक्ष हैं। यह एक मेटा नहीं है...
draft: false
keywords:
- मन-के-वेक्टर
- पांच
- शब्द
- वेक्टर
lang: hi
lastmod: '2025-07-09'
license: https://creativecommons.org/licenses/by-sa/4.0/
original_id: '50351822'
original_url: https://www.vectorsofmind.com/p/the-big-five-are-word-vectors
quality: 6
slug: the-big-five-are-word-vectors
tags: []
title: बिग फाइव हैं वर्ड वेक्टर
translation_model: gpt-4o
---

*From [Vectors of Mind](https://www.vectorsofmind.com/p/the-big-five-are-word-vectors) - images at original.*

---

मनोविज्ञान में शब्दकोशीय अध्ययन और कंप्यूटर विज्ञान में लेटेंट सेमांटिक एनालिसिस को अलग-अलग समस्याओं को हल करने के लिए आधी सदी के अंतराल पर पेश किया गया था, फिर भी वे गणितीय रूप से समकक्ष हैं। यह एक रूपक नहीं है जो एक निश्चित स्तर के अमूर्तता पर काम करता है _;_ बिग फाइव [शब्द वेक्टर](https://dzone.com/articles/introduction-to-word-vectors) के आयाम हैं।

लेकिन पहले, कुछ पृष्ठभूमि। शब्दकोशीय परिकल्पना का दावा है कि व्यक्तित्व संरचना भाषा में लिखी जाएगी क्योंकि वक्ताओं को अपने आसपास के सबसे प्रमुख गुणों का वर्णन करना होता है। इस विचार की सुंदरता यह है कि, किसी एक व्यक्ति के व्यक्तित्व के मॉडल का सुझाव देने के बजाय, भाषा यह रिकॉर्ड करती है कि लाखों लोग किसे उपयोगी मानते हैं। मनोमिति विज्ञानी का काम बस इस संरचना की पहचान करना है। आमतौर पर इसे मनोविज्ञान के छात्रों को विशेषणों की सूची पर खुद को रेट करने के लिए आमंत्रित करके और सहसंबंध मैट्रिक्स पर फैक्टर विश्लेषण करके पूरा किया गया है। 1933 में, एलएल थर्स्टन ने 1300 लोगों को 60 विशेषणों का सर्वेक्षण दिया। अपने महत्वपूर्ण [द वेक्टर्स ऑफ माइंड](http://psych.colorado.edu/~carey/Courses/PSYC5112/Readings/VectorsOfMind_Thurstone.pdf) में, वे रिपोर्ट करते हैं कि "पाँच कारक पर्याप्त हैं" डेटा को समझाने के लिए। बाद के दशकों में, ऐसे अध्ययन, कमोबेश, पाँच प्रमुख घटकों में परिणत हुए: सहमति, बहिर्मुखता, कर्तव्यनिष्ठा, तंत्रिकातंत्रता, और खुलापन/बुद्धि। (विषय पर उत्कृष्ट उपचार के लिए, देखें [बिग फाइव के शब्दकोशीय नींव](https://www.researchgate.net/profile/Boele-Raad-2/publication/282980275_The_Lexical_Foundation_of_the_Big_Five-Factor_Model/links/5626198508aed3d3f137e522/The-Lexical-Foundation-of-the-Big-Five-Factor-Model.pdf)।)

लेटेंट सेमांटिक एनालिसिस को 1988 में एक सूचना पुनर्प्राप्ति तकनीक के रूप में [पेश किया गया](https://dl.acm.org/doi/abs/10.1145/57167.57214?casa_token=ogUyQ6VJeZgAAAAA:ksULYwu-Km_5Ap0wA2ho3tbwzTjsB0tHONfEEMIldNB6PJgkRyM7eFaa7uZ-XZJ3nYo0MbYFeJsBng)। शब्दों को वेक्टर के रूप में प्रस्तुत किया जा सकता है और दस्तावेज़ों या वाक्यों को उनके शब्द वेक्टर के औसत के रूप में प्रस्तुत किया जा सकता है। यदि आप एक बड़े डेटाबेस (जैसे, विकिपीडिया) में खोज करना चाहते हैं, तो प्रत्येक पृष्ठ के लिए कीवर्ड आपको केवल इतना ही आगे ले जा सकते हैं। इससे बचने का एक तरीका यह है कि दस्तावेज़ों (विकी लेख) और क्वेरी (खोज शब्द) दोनों को उनके शब्द वेक्टर के औसत के रूप में प्रस्तुत किया जाए। प्रासंगिक दस्तावेज़ों को खोजना अब एक साधारण डॉट प्रोडक्ट के साथ पूरा किया जा सकता है। (इस पोस्ट में, मैं LSA और शब्द वेक्टर को समानार्थी मानता हूँ। भाषा को वेक्टराइज़ करने और विशेष रूप से शब्द वेक्टर बनाने के अन्य तरीके भी हैं, लेकिन वे अभी के लिए दायरे से बाहर हैं।)

उनके अलग-अलग उपयोग और इतिहास के बावजूद, चरण समान हैं:

1. एक शब्द x दस्तावेज़ गणना मैट्रिक्स एकत्र करें

2. गैर-रेखीय परिवर्तन

3. मैट्रिक्स अपघटन

4. रोटेशन (वैकल्पिक)

परिणाम एक सेट के शब्द वेक्टर हैं जो प्रत्येक शब्द का संक्षेप में वर्णन करते हैं। इनका उपयोग कई डाउनस्ट्रीम कार्यों के लिए किया जा सकता है, जैसे भावना विश्लेषण से लेकर छात्र निबंधों से आत्ममुग्धता की भविष्यवाणी तक। व्यक्तित्व विशेषणों के मामले में, शब्द वेक्टर के आयामों का विश्लेषण, नामकरण और दशकों तक बहस की गई। जो आगे आता है वह प्रत्येक चरण में अंतर की चर्चा है।

**गणना मैट्रिक्स।** LSA आमतौर पर विभिन्न प्रकार के दस्तावेज़ों की एक बड़ी संख्या (उदाहरण के लिए, लाखों अमेज़न उत्पाद समीक्षाएँ) शामिल करता है। इन्हें शब्द x डॉक मैट्रिक्स में बदल दिया जाता है, यह गिनकर कि प्रत्येक डॉक में प्रत्येक शब्द कितनी बार प्रकट होता है। मनोविज्ञान में, एक दस्तावेज़ वे शब्द होते हैं जिनसे एक विषय सहमत होता है जो उन्हें वर्णित करता है। यह लाइकेर्ट स्केल तक भी विस्तारित होता है। यदि कोई कहता है कि कोई शब्द उन्हें 5/7 वर्णित करता है, तो बस उस शब्द को दस्तावेज़ में पाँच बार दोहराएं।

**गैर-रेखीय परिवर्तन।** शब्दकोशीय अध्ययन अक्सर डेटा को इप्सेटाइज करते हैं (विषय अक्ष के साथ z स्कोर) और फिर पियर्सन सहसंबंध करते हैं। थर्स्टन ने अपने अध्ययन में एक पुरातन टेट्रार्क सहसंबंध का उपयोग किया। LSA में, सबसे आम परिवर्तन TF-IDF है, उसके बाद लघुगणक। यह सुनिश्चित करता है कि मैट्रिक्स सामान्य शब्दों द्वारा हावी न हो। अक्सर, परिवर्तन एक शब्द x शब्द संबंध मैट्रिक्स (उदाहरण के लिए, सहसंबंध मैट्रिक्स) में परिणत होता है। यह कदम व्यावहारिक रूप से बहुत महत्वपूर्ण है लेकिन इतना सैद्धांतिक नहीं है। चुनने के लिए परिवर्तन वह है जो अंत में आपको एक उचित परिणाम देता है।

**मैट्रिक्स अपघटन।** मैट्रिक्स अपघटन के कई तरीके हैं। कुछ, जैसे PCA, एक वर्ग मैट्रिक्स की आवश्यकता होती है। अन्य लापता डेटा के प्रति मजबूत होते हैं। व्यक्तित्व डेटा के साथ, पसंद का कोई खास फर्क नहीं पड़ता; परिणाम बहुत समान होते हैं। सामान्य शब्द वेक्टर के लिए किसी शब्द के अर्थ, भाषण के भाग, स्लैंगनेस और अन्य बहुत कुछ जो भाषा को बनावट देता है, का प्रतिनिधित्व करने के लिए ~300 आयामों की आवश्यकता होती है। चूंकि सर्वेक्षण को उस स्थिरता को बनाए रखने के लिए डिज़ाइन किया गया है, केवल ~5 आयामों की आवश्यकता होती है। थर्स्टन ने अपने पांच के चयन को पुनर्निर्माण त्रुटि को देखकर उचित ठहराया जिसे वह एक हिस्टोग्राम के रूप में रिपोर्ट करते हैं। बाद के मनोवैज्ञानिकों ने पुनर्निर्माण त्रुटि (स्वयंमानों के साथ मापी गई), साथ ही व्याख्या और पुनरुत्पादन क्षमता पर विचार करके 5 को उचित ठहराया।

[*[छवि: मूल पोस्ट से दृश्य सामग्री]*](https://substackcdn.com/image/fetch/$s_!Zw-J!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd562c1c2-1576-4fad-896c-52e799d4598b_1600x1066.png) शब्द सहसंबंध मैट्रिक्स की पुनर्निर्माण त्रुटि। कम्प्यूटेशनल बाधाओं के बावजूद उनका नमूना कई आधुनिक अध्ययनों की तुलना में एक क्रमिक रूप से बड़ा है।

**रोटेशन।** क्या आपने कभी घटक अधिक-उत्खनन के बारे में सुना है? यह कोई कहानी नहीं है जो मनोवैज्ञानिक आपको बताएंगे। यह तब होता है जब कोई शोधकर्ता बहुत अधिक प्रिंसिपल कंपोनेंट्स निकालता है और फिर पहले के, मान्य पीसी से बाद के सीमांत पीसी पर विचरण को घुमाता है। विश्वास करें या न करें, बिग फाइव के साथ ऐसा ही हुआ! जो अब सहमति है वह कभी एक बहुत अधिक मजबूत और सैद्धांतिक रूप से संतोषजनक 'सामाजिककरण' कारक था, जिसे कर्तव्यनिष्ठा, तंत्रिकातंत्रता और खुलापन बनाने के लिए पीसी 3-5 पर फैलाया गया था। व्याख्यात्मक कारकों का उत्पादन करने के लिए रोटेशन _सही_ ठहराया जा सकता है। लेकिन अगर आप कभी खुद को घुमाते हुए और सही कारकों की संख्या के बारे में बहस करते हुए पाते हैं, तो खुद को जांचें!

**शब्द वेक्टर से बिग थ्री**

मैंने फेसबुक स्टेटस से बिग फाइव लक्षणों की भविष्यवाणी करते हुए अपनी पीएचडी शुरू की। जब मैंने पढ़ा कि व्यक्तित्व सॉसेज कैसे बनाया गया था, तो मुझे एहसास हुआ कि परियोजना ने बिग फाइव स्पेस में व्यक्तियों के रहने के शोर वाले अनुमानों की भविष्यवाणी करने के लिए शब्द वेक्टर (फेसबुक स्टेटस के) का उपयोग किया, जिसे मूल रूप से शब्द वेक्टर द्वारा परिभाषित किया गया था। ऐसा लग रहा था कि सीधे मुद्दे पर आना और शब्द वेक्टर से व्यक्तित्व के बारे में कुछ मौलिक सीखना अधिक दिलचस्प होगा। (इसके अलावा, मैं जिस डेटासेट का उपयोग कर रहा था वह कैम्ब्रिज एनालिटिका के बाद विषाक्त हो गया।) मेरी पीएचडी का बाकी हिस्सा बिग फाइव को पुन: उत्पन्न करने के लिए शब्द वेक्टर को बाधित करने पर काम कर रहा था। इसमें LSA के बजाय ट्रांसफॉर्मर का उपयोग करना शामिल था (इस पर भविष्य की पोस्ट में अधिक)। शब्द वेक्टर (DeBERTa) बनाम सर्वेक्षणों से कारकों के बीच परिणामी सहसंबंध नीचे है। जैसा कि आप देख सकते हैं, पहले तीन कारकों के लिए बहुत करीबी समझौता है। जहां परिणाम भिन्न होते हैं, यह स्पष्ट नहीं है कि कौन सी विधि में त्रुटि है। शायद सर्वेक्षण सही हैं, और जब हमें GPT-5 मिलेगा तो सभी सहसंबंध 1 पर चले जाएंगे। शायद सर्वेक्षण सिर्फ पक्षपाती और शोर वाले हैं, और बहुत अधिक पीसी निकाले गए थे। शायद वे अलग-अलग चीजों को माप रहे हैं, और हमें दोनों की अपनी व्याख्या को परिष्कृत करने की आवश्यकता है। किसी भी दर पर, यह मेरे लिए स्पष्ट नहीं है कि सर्वेक्षणों को दोनों के बीच स्वर्ण मानक माना जाना चाहिए। आखिरकार, शब्दकोशीय परिकल्पना भाषा संरचना के बारे में है, और मनोविज्ञान एकमात्र क्षेत्र है जो प्राकृतिक भाषा का विश्लेषण करने के लिए सर्वेक्षण का उपयोग करता है।

[*[छवि: मूल पोस्ट से दृश्य सामग्री]*](https://substackcdn.com/image/fetch/$s_!lY1-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6bf087b4-76cc-4272-a2f7-037d606ed2ba_726x682.png) बिना घुमाए गए सर्वेक्षण पीसी उन [अध्ययनों](https://onlinelibrary.wiley.com/doi/10.1002/\(SICI\)1099-0984\(199603\)10:1%3C61::AID-PER246%3E3.0.CO;2-D) में से एक से हैं जिन्होंने बिग फाइव को परिभाषित किया। DeBERTa पीसी शब्द वेक्टर से निकाले गए हैं। उस प्रक्रिया के बारे में [यहाँ पढ़ें](https://psyarxiv.com/gdm5v/)।

**निष्कर्ष**

थर्स्टन ने 30 के दशक में शब्दकोशीय परिकल्पना की जांच के लिए सांख्यिकी और रैखिक बीजगणित में तरीकों का बीड़ा उठाया। यह उल्लेखनीय है कि उन्होंने शब्दों का प्रतिनिधित्व करने का एक तरीका विकसित किया जिसे बाद में सूचना पुनर्प्राप्ति के लिए फिर से खोजा गया, जो अब सूचना युग को शक्ति प्रदान करता है। गणना ने थर्स्टन को भाषा के समृद्ध परिदृश्य को सर्वेक्षण प्रतिक्रियाओं में समतल करने के लिए मजबूर किया। पिछले 30 वर्षों में एनएलपी ने कई क्रांतियों का अनुभव किया है। यदि थर्स्टन ने भाषा संरचना को देखने के लिए एक दूरबीन का आविष्कार किया, तो अब हमारे पास हबल है। कई अंतर्दृष्टियाँ प्रतीक्षा कर रही हैं!