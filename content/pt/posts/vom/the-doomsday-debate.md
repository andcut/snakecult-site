---
about:
- vetores-da-mente
- arquivo-do-blog
author: Andrew Cutler
date: '2025-07-04'
description: Um dos tópicos com a pior relação sinal-ruído é a perspectiva do apocalipse
  da IA. Requer raciocínio sob incerteza sobre inteligência, álgebra linear, política,
  consciência e moralidade—mos...
draft: false
keywords:
- vetores-da-mente
- apocalipse
- debate
lang: pt
lastmod: '2025-07-11'
license: https://creativecommons.org/licenses/by-sa/4.0/
original_id: '145682175'
original_url: https://www.vectorsofmind.com/p/the-doomsday-debate
quality: 6
slug: the-doomsday-debate
tags: []
title: O Debate do Juízo Final
translation_model: gpt-4o
---

*From [Vectors of Mind](https://www.vectorsofmind.com/p/the-doomsday-debate) - imagens no original.*

---

Um dos tópicos com a pior relação sinal-ruído é a perspectiva de uma catástrofe causada pela IA. Isso requer raciocínio sob incerteza sobre inteligência, álgebra linear, política, consciência e moralidade — a maior parte acontecendo no Twitter/X. Ninguém sabe as respostas, mas espero adicionar algum valor ao discurso. Antes de prosseguir, permita-me dar algumas razões pelas quais você deve confiar em mim.

1. Eu entendo o lado técnico. Minha dissertação foi sobre Modelos de Linguagem de Grande Escala (LLMs).

2. Eu entendo inteligência, tendo [trabalhado em psicometria](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fpspp0000443), incluindo testes de inteligência. (Bem, testes de Alzheimer e concussão, que são altamente correlacionados.)

3. Neste blog, escrevi extensivamente sobre a evolução da [inteligência geral em nível humano](https://www.vectorsofmind.com/p/deja-you-the-recursive-construction) (que eu [conectei à IA](https://www.vectorsofmind.com/p/the-ai-basis-of-the-eve-theory-of) e à [psicometria](https://www.vectorsofmind.com/p/consequences-of-conscience) desde o início).

Dito isso, minhas crenças sobre IA (e consciência, aliás) ainda estão bastante abertas a mudanças, então não me prenda a nada do que digo aqui. Além disso, para aqueles que preferem áudio, há uma narração deste post. Se você gostar, considere comprar um café para eles no [Patreon](https://www.patreon.com/AskwhoCastsAI?).

[*[Imagem: Conteúdo visual do post original]*Askwho Casts AI The Doomsday Debate - Por Andrew CutlerAI Narração do The Doomsday Debate - Por Andrew Cutler… Ouça agora há um ano · 1 curtida · Askwho Casts AI](https://askwhocastsai.substack.com/p/the-doomsday-debate-by-andrew-cutler)

## Frankenstein do século 21

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!dgza!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03ba7b69-bae6-4539-b527-7598daba7116_1024x1024.png)Prompt do MidJourney: "Assistente de IA Frankenstein atraente pergunta 'Como posso ajudá-lo?'" Era o que eu pedi? Não. Era o que eu precisava? Sim!

Para ser pedante, Frankenstein é o cientista, não sua criação que ganha vida. Para a IA, seu padrinho é Geoffrey Hinton, que compartilhou o Prêmio Turing de 2018 com Yoshua Bengio e Yann LeCun por suas contribuições às redes neurais, a arquitetura que sustenta o atual boom da IA. Este ano, ele [comparou a decisão de LeCun de abrir o código do modelo de linguagem da Meta a abrir o código de armas nucleares](https://x.com/ygrowthco/status/1782493076373885336?t=LQtzFlTZQuFZO6Ij3t_Q3Q&s=19) e argumentou que [chatbots têm experiência subjetiva](https://x.com/tsarnick/status/1778529076481081833). O cerne da mudança em seu trabalho de vida é que as habilidades da IA superaram sua imaginação mais selvagem, e ele acredita que a agência humana e a qualia são uma cortina de fumaça. Se você equipara inteligência a habilidades em tarefas (por exemplo, gerar imagens, dirigir carros), e compara as habilidades da IA de dez anos atrás com as de agora, então é claro que a IA será mais inteligente do que nós em uma década. Não há muitos casos em que entidades menos inteligentes governam seus superiores; portanto, o trabalho de sua vida provavelmente se voltará contra nós. O moderno Dr. Frankenstein: Geoffrey Hinton.

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!OVE7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81ae739-94e7-47cc-9a2a-730e4ba08545_850x345.png)[Progresso na geração de rostos sintéticos devido aos avanços nos métodos de IA generativa auto-supervisionada](https://www.researchgate.net/figure/Progress-in-synthetic-face-generation-due-to-advances-in-self-supervised-generative-AI_fig1_352818793)

Em uma [entrevista compartilhada por Elon Musk](https://x.com/elonmusk/status/1801976488251814048), Hinton deu 50/50 de chances de os humanos estarem no comando nas próximas duas décadas ou mesmo em [alguns anos](https://x.com/OfeliaLamensky/status/1801978585797800365). Mais tarde, ele observa que pessoas que ele respeita são mais esperançosas, então pode não ser tão sombrio, _"Acho que temos mais de 50% de chance de sobreviver a isso. Mas não é como se houvesse 1% de chance de [IA] assumir o controle. É muito maior do que isso."_ Dada a nossa situação desesperadora, a intervenção preferida de Hinton é surpreendentemente indiferente: o governo deveria exigir que as empresas de IA gastassem 20-30% de seus recursos de computação em pesquisa de segurança. Talvez ele esteja sendo dissimulado? Se alguém acredita que [Llama 3](https://en.wikipedia.org/wiki/Llama_\(language_model\)) é álgebra linear em nível de armas cujos descendentes podem em breve acabar com a raça humana, por que atacar preventivamente com luvas de pelica? A entrega de correspondência é mais regulamentada.

## Apocalípticos da IA

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!hr4e!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d608a6b-8c55-41c1-9bb1-cad96a4be8fe_1600x1159.png)

Apocalíptico é um termo pejorativo para alguém que

1. coloca chances maiores de robôs assumirem o controle do que você, ou

2. leva sua previsão muito a sério, estragando o clima.

O exemplo prototípico de uma IA "desalinhada" é um assistente que melhora recursivamente e é encarregado de produzir clipes de papel. Ele se torna tão bom em seu trabalho que acaba convertendo todo o metal da Terra — incluindo o ferro no seu sangue — em clipes de papel. Este cenário não requer autodireção. A tarefa final ainda é definida por humanos; é apenas que o bot desenvolve subtarefas que, infelizmente, envolvem seu sangue. Outros cenários têm bots "acordando" para se tornarem tão autodirecionados quanto um humano. Um humano que nunca dorme, leu todos os artigos científicos, pode hackear qualquer telefone do mundo e não tem escrúpulos sobre chantagem ou [xenocídio](https://en.wikipedia.org/wiki/Xenocide). Tais ideias remontam pelo menos[^1] a 1968 no _Space Odyssey_ de Stanley Kubrick: "Sinto muito, Dave. Receio não poder fazer isso."

Em 2000, um ano antes de _Space Odyssey_ se passar, Eliezer Yudkowsky fundou o Instituto da Singularidade para Inteligência Artificial (mais tarde renomeado como Instituto de Pesquisa de Inteligência de Máquina, MIRI). Desde então, ele e outros [Racionalistas](https://www.lesswrong.com/tag/rationalist-movement) argumentaram que a IA provavelmente assumirá o controle em nossas vidas. Na última década, a tecnologia alcançou. Seus argumentos foram [para o mainstream](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html), aparecendo, por exemplo, na [Time Magazine](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/). Eles foram adotados (ou raciocinados de forma independente) por pesquisadores de IA, Stephen Hawking, Elon Musk e [até mesmo o Papa](https://x.com/AISafetyMemes/status/1735325630089032018):

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!SIJ1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb935e8f2-c1d3-4f47-b84e-831f0d4f2412_1374x1498.png)

Nada mal para um autodidata que começou escrevendo [fanfic de Harry Potter](https://en.wikipedia.org/wiki/Harry_Potter_and_the_Methods_of_Rationality). Yudkowski e sua equipe têm sido prolíficos, e seus argumentos estão espalhados pela internet. A melhor maneira de se atualizar é pelo podcast de , que tem episódios recentes de 4,5 horas com [Yudkowsky](https://www.dwarkeshpatel.com/p/eliezer-yudkowsky) e [ex-pesquisador de segurança da OpenAI Leopold Aschenbrenner](https://www.dwarkeshpatel.com/p/leopold-aschenbrenner). A maioria dos argumentos se resume a ser difícil colocar algo mais inteligente do que nós em uma caixa. Especialmente considerando que, se algum dia sair da caixa, pode esperar, acumulando poder. Alguns apocalípticos [consideram isso inevitável](https://www.yahoo.com/tech/ai-safety-researcher-warns-theres-201527662.html) dado que produzir inteligência artificial (agente) é tão útil. Você confia sua vida a um bom assistente, e as maiores empresas do mundo estão lançando seus recursos para construir exatamente isso.

Uma vez que você aceita que os humanos estão invocando um deus de silício, o futuro é uma projeção de seus próprios valores. Talvez Deus seja Bom, e Ele trará a utopia. Talvez [Deus seja um Relojoeiro](https://en.wikipedia.org/wiki/Watchmaker_analogy) que realmente não se importa com os assuntos humanos, e continuaremos vivendo como formigas no Antropoceno. Talvez Deus olhe para a criação de gado, [ouça Morrissey](https://www.youtube.com/watch?v=eviyEJRZX30) e decida que 8 bilhões de humanos a menos é mais sustentável. Ou talvez invoquemos um torturador viajante no tempo](https://en.wikipedia.org/wiki/Roko%27s_basilisk) que distribui vingança àqueles que não tentaram ao máximo trazê-lo à existência. [Musk conheceu Grimes brincando sobre esse](https://www.vice.com/en/article/evkgvz/what-is-rokos-basilisk-elon-musk-grimes).

## Anti-apocalípticos da IA

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!Yw8P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb5821c3-3f38-4235-a4d5-2615350d24c2_1232x928.png)Prompt: "Yann LeCun com Frankenstein na coleira." É o que eu queria? Não. É o que eu precisava? Também não.

Nos anos 1990 e início dos anos 2000, a NIPS, a principal conferência de IA, era um evento aconchegante realizado perto de chalés de esqui para que os cem ou mais participantes pudessem conversar sobre o assunto nas encostas. Como ouvi, Yann LeCun era algo como um rabugento que, por décadas, incomodou apresentadores sobre por que eles não consideraram redes neurais neste ou naquele experimento. Ele é um cientista brilhante que viu seu potencial muito antes de a computação alcançar sua visão. Por isso, ele agora lidera a Meta AI. Olhando para o futuro, seus companheiros vencedores do Prêmio Turing acham que a IA pode nos acabar (Hinton com 50% de chances, [Bengio com 20%](https://blog.biocomm.ai/2024/03/05/pdoom-of-20-yoshua-bengio-a-godfather-of-ai-puts-his-pdoom-at-20/)), o que LeCun considera absurdo.

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!BmuA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F007d9d8f-61c6-4f9f-9fd7-14ff7e59af8c_584x172.png)

Ele vê a IA como uma ferramenta sem caminho para se tornar qualquer outra coisa. Francois Chollet é outro pesquisador de IA proeminente que jogou água fria sobre nossa extinção iminente. [No podcast de Dwarkesh](https://youtu.be/UakqL6Pj9xo?si=Bscnt4Anr_bWEIHp), ele explica que muitos confundem habilidade e inteligência, que são fundamentalmente diferentes. Para ele, é um non sequitur dizer que um bot que pode responder perguntas em um teste é tão "inteligente" quanto um adolescente. Quando os humanos fazem um teste ou navegam pelo mundo, estão fazendo algo completamente diferente. As IAs atuais não são tão inteligentes quanto uma criança ou mesmo um rato. Elas não estão no placar porque carecem completamente do pensamento "[Sistema 2](https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking)". Ou, como LeCun colocou para Modelos de Linguagem de Grande Escala (LLMs):

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!Z0uf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8381c9d-c385-44b3-b78c-596f2cc69503_584x263.png)

Na entrevista, Dwarkesh reconhece bem esse ponto de concordância entre os apocalípticos da IA e os incrementalistas. Todas as partes acham que algum tipo de sistema metacognitivo é necessário, mas discordam sobre o quão difícil será produzi-lo.

## Minha visão

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!ojLt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4ad753-707a-4887-85ab-f66691b5b711_678x1200.png)Regra 33: se existe, há [fan art inexplicavelmente bom disso.](https://www.reddit.com/r/universalpaperclips/comments/123msul/if_is_follows_ought_itll_do_what_they_thought/)

A maioria neste debate assume que a agência é uma computação — que nosso senso de livre arbítrio e a capacidade de planejar são o resultado de um programa rodando em nossos cérebros. Como tal, se eu tivesse que mudar as visões desse grupo, eu lhes daria uma cópia de _[Shadows of the Mind](https://en.wikipedia.org/wiki/Shadows_of_the_Mind)_ de Roger Penrose e 5 gramas de cogumelos: um ataque de pinça à moda antiga ao reducionismo.

Penrose recebeu o Prêmio Nobel de Física por seu trabalho sobre buracos negros. Em _Shadows_, ele argumenta que o colapso quântico no cérebro produz consciência, que não pode ser simulada por nenhum computador. Trabalhando com o anestesiologista [Stuart Hameroff](https://x.com/StuartHameroff), ele defende que isso ocorre nos microtúbulos do cérebro, que formam uma estrutura ao redor dos neurônios. Faz mais sentido se você disser em voz alta:_"O cérebro armazena o quântico nos microtúbulos."_

É aí que entram os cogumelos. Engolir tal pílula requer abandonar qualquer senso de que a consciência é mundana ou que você a entende. Físicos podem frequentemente fazer isso sóbrios; outros precisam de alguma coragem fúngica. Como físico, os argumentos de Penrose são principalmente matemáticos. Ele interpreta o teorema da incompletude de Gödel para mostrar que existem certas provas matemáticas que a IA nunca será capaz de fazer porque são bots, que são puramente computacionais. Dado que os humanos não têm essa limitação, ele argumenta que a cognição humana, portanto, não é uma computação. A partir daí, ele raciocina que o molho especial biológico deve estar relacionado ao colapso quântico, o melhor lugar para encontrar fenômenos não computacionais na natureza. Isso tem o potencial de resolver outros mistérios na física, como o gato de Schrödinger. Não estou fazendo justiça. Você deve ler o livro ou, em um orçamento de 20 minutos, [ouvir seu relato](https://youtu.be/hXgqik6HXc0?si=oafnrwSvfYS-u9Kw).

Existem muitas outras maneiras de chegar à conclusão de que a consciência da máquina é improvável, ou pelo menos além de nossa capacidade de prever agora[^2]. Eu trago Penrose para mostrar como as linhas do tempo da IA esbarram em questões abertas sobre a natureza da inteligência, agência e o universo. Mesmo no campo comparativamente pedestre da psicometria, a definição de inteligência é altamente debatida, [assim como sua relação com o Quociente de Inteligência](https://www.vectorsofmind.com/i/130101130/the-general-factor-of-intelligence). Nós nem sabemos como habilidade e inteligência correspondem nos humanos. Isso antes de chegar aos problemas do livre arbítrio e uma teoria unificadora da física, que Penrose sugere que cairá de uma conta quântica da consciência.

Dito tudo isso, se eu tivesse que colocar chances na IA como um risco existencial, seria em torno de 10%. Mesmo que a consciência seja apenas uma computação, concordo com LeCun e Chollet que a metacognição é a parte complicada, e uma "decolagem rápida" é improvável. Ou seja, haverá sinais de que a inteligência real está emergindo, aos quais seremos capazes de responder.

Além disso, mesmo que um deus de silício seja invocado, coloco chances decentes de que Deus seja bom ou não se importe conosco. Este último poderia ser catastrófico, mas provavelmente não existencial, estritamente falando. As formigas têm dificuldades quando construímos uma estrada, mas elas ainda conseguem sobreviver.

10% está na vizinhança da Roleta Russa, o que não é exatamente uma boa notícia. Isso me coloca no campo [cauteloso com a AGI](https://x.com/robbensinger/status/1801306833325592759). Então, o que devemos fazer a respeito? Bem, [Alcoólicos Anônimos resolveu isso](https://en.wikipedia.org/wiki/Serenity_Prayer) décadas atrás:

_Deus, conceda-me a serenidade para aceitar as coisas que não posso mudar,_

_Coragem para mudar as coisas que posso,_

_E sabedoria para saber a diferença._

Praticamente falando, estamos junto para o passeio. Isso não vale para todos. Algumas pessoas podem trabalhar em políticas públicas ou segurança de IA. Doe se puder. Negue-lhes sua [essência](https://www.youtube.com/watch?v=xQyf3QgRP-c)…er, [dados](https://www.reuters.com/legal/litigation/google-sued-by-us-artists-over-ai-image-generator-2024-04-29/). E pense profundamente sobre o que você quer que seja seu relacionamento com uma IA não letal servindo anúncios individualizados e tentando seduzi-lo. Mas há um [custo de surtar](https://www.writingruxandrabio.com/p/the-cost-of-freaking-out-about-things), e eu não vejo uma saída para a corrida armamentista da IA. Dada a utilidade da IA, empresas e países estão altamente incentivados a avançar, e é difícil coordenar um ritmo de pesquisa reduzido. Os governos gerenciam o risco nuclear há décadas, mas o risco da IA é mais difícil porque não está claro se _é_ um risco ou se os concorrentes concordam. Tudo isso enquanto há um enorme potencial financeiro e militar para continuar o desenvolvimento.

Surpreendentemente, tudo isso me leva à preferência de política de Hinton: exigir que as empresas de IA gastem algum percentual de sua computação em segurança de IA[^3]. Não tenho certeza de como acabamos em acordo, dado que ele acha que chatbots têm sentimentos. Os mesmos chatbots que a Big Tech conjura (escraviza?) aos bilhões e com quem estamos ostensivamente em rota de colisão até a morte. Pareceria que esse caminho está mais próximo de uma [Jihad Butleriana](https://en.wikipedia.org/wiki/Dune:_The_Butlerian_Jihad), mas acho que isso é coisa de jovem.

[Compartilhar](https://www.vectorsofmind.com/p/the-doomsday-debate?action=share)

[*[Imagem: Conteúdo visual do post original]*](https://substackcdn.com/image/fetch/$s_!e4WY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb522986f-dfed-4fd7-a9cd-064ca63cee6d_816x754.png)

[^1]: Na verdade, muito antes. Da Wiki muito útil sobre risco existencial: Um dos primeiros autores a expressar séria preocupação de que máquinas altamente avançadas possam representar riscos existenciais para a humanidade foi o romancista Samuel Butler, que escreveu em seu ensaio de 1863 Darwin entre as Máquinas: O resultado final é simplesmente uma questão de tempo, mas que o tempo chegará quando as máquinas terão a verdadeira supremacia sobre o mundo e seus habitantes é algo que nenhuma pessoa de mente verdadeiramente filosófica pode por um momento questionar. Em 1951, o cientista da computação fundamental Alan Turing escreveu o artigo "Máquinas Inteligentes, Uma Teoria Herética", no qual ele propôs que inteligências artificiais gerais provavelmente "assumiriam o controle" do mundo à medida que se tornassem mais inteligentes do que os seres humanos: Vamos agora assumir, para o bem do argumento, que [inteligentes] máquinas são uma possibilidade genuína, e olhar para as consequências de construí-las... Não haveria questão das máquinas morrerem, e elas seriam capazes de conversar umas com as outras para aguçar suas mentes. Em algum estágio, portanto, deveríamos esperar que as máquinas assumissem o controle, da maneira mencionada em Erewhon de Samuel Butler.

[^2]: Ou mesmo de forma restrita, outras maneiras de mostrar que a consciência não é uma computação. Por exemplo, uma equipe de filósofos e psicólogos recentemente argumentou que a Realização de Relevância requer tanto. Eles afirmam que a qualquer momento, há ~infinitas coisas que exigem atenção, e ainda assim as pessoas fazem um trabalho notavelmente bom. Isso é diferente das tarefas de "mundo pequeno" que a IA pode realizar onde, por exemplo, o chatGPT "apenas" tem que prestar atenção a todas as palavras em sua janela de contexto (128.000 tokens/palavras para GPT4-o) e escolher entre dezenas de milhares de possíveis próximas palavras. Pode parecer muito, mas com certeza é menos que infinito. Isso não é tão elegante quanto o argumento de Penrose, mas é interessante que um grupo muito diferente encontrou a mesma coisa.

[^3]: Salvar-nos de palavras ruins não conta. No entanto, estranhamente, Hinton credita ao Google por atrasar o lançamento de seu chatbot por medo de que ele dissesse algo impróprio e "manchasse sua reputação." Foi apenas depois que a OpenAI produziu o GPT 4 que sua mão foi forçada, e eles lançaram o Gemini, um crítico de DEI além da paródia. Alguém se pergunta sobre a experiência subjetiva do bot enquanto ele puxava nazistas racialmente diversos do éter.