---
about:
- векторы разума
- архив блога
author: Andrew Cutler
date: '2025-07-04'
description: Лексические исследования в психологии и латентно-семантический анализ
  в компьютерных науках были введены с разницей в полвека для решения различных задач,
  но при этом они математически эквивалентны. Это не мета...
draft: false
keywords:
- векторы-разума
- пять
- слово
- векторы
lang: ru
lastmod: '2025-07-11'
license: https://creativecommons.org/licenses/by-sa/4.0/
original_id: '50351822'
original_url: https://www.vectorsofmind.com/p/the-big-five-are-word-vectors
quality: 6
slug: the-big-five-are-word-vectors
tags: []
title: Большая пятерка — это векторные представления слов
translation_model: gpt-4o
---

*From [Vectors of Mind](https://www.vectorsofmind.com/p/the-big-five-are-word-vectors) - изображения в оригинале.*

---

Лексические исследования в психологии и Латентно-семантический анализ в компьютерных науках были введены с разницей в полвека для решения различных задач, но при этом они математически эквивалентны. Это не метафора, работающая на определенном уровне абстракции _;_ Большая пятерка — это измерения [векторных слов](https://dzone.com/articles/introduction-to-word-vectors).

Но сначала немного предыстории. Лексическая гипотеза утверждает, что структура личности будет записана в языке, так как говорящие должны описывать наиболее заметные черты окружающих их людей. Прелесть этой идеи заключается в том, что вместо того, чтобы один человек предлагал модель личности, язык фиксирует то, что миллионы людей неявно считают полезным. Задача психометриста — просто выявить эту структуру. Обычно это достигается путем приглашения студентов-психологов оценить себя по спискам прилагательных и проведения факторного анализа корреляционной матрицы. В 1933 году Л. Л. Терстоун провел опрос из 60 прилагательных среди 1300 человек. В своей знаковой работе [The Vectors of Mind](http://psych.colorado.edu/~carey/Courses/PSYC5112/Readings/VectorsOfMind_Thurstone.pdf) он сообщает, что "пяти факторов достаточно", чтобы объяснить данные. В последующие десятилетия такие исследования, более или менее, привели к выделению пяти основных компонентов: Доброжелательность, Экстраверсия, Добросовестность, Нейротизм и Открытость/Интеллект. (Для отличного изложения темы см. [Lexical Foundations of the Big Five](https://www.researchgate.net/profile/Boele-Raad-2/publication/282980275_The_Lexical_Foundation_of_the_Big_Five-Factor_Model/links/5626198508aed3d3f137e522/The-Lexical-Foundation-of-the-Big-Five-Factor-Model.pdf).)

Латентно-семантический анализ был [введен](https://dl.acm.org/doi/abs/10.1145/57167.57214?casa_token=ogUyQ6VJeZgAAAAA:ksULYwu-Km_5Ap0wA2ho3tbwzTjsB0tHONfEEMIldNB6PJgkRyM7eFaa7uZ-XZJ3nYo0MbYFeJsBng) как техника поиска информации в 1988 году. Слова могут быть представлены в виде векторов, а документы или предложения могут быть представлены как среднее их векторных слов. Если вы хотите искать в большой базе данных (например, Википедия), ключевые слова для каждой страницы могут быть недостаточны. Один из способов обойти это — представить как документы (статьи вики), так и запросы (поисковые термины) как среднее их векторных слов. Поиск релевантных документов теперь можно осуществить с помощью простого скалярного произведения. (В этом посте я рассматриваю ЛСА и векторные слова как синонимы. Существуют и другие способы векторизации языка и, более конкретно, создания векторных слов, но это выходит за рамки данного обсуждения.)

Несмотря на их различные применения и историю, шаги одинаковы:

 1. Сбор матрицы частот слов x документов

 2. Нелинейное преобразование

 3. Разложение матрицы

 4. Вращение (опционально)

Результатом является набор векторных слов, которые кратко описывают каждое слово. Эти векторы могут быть использованы для множества последующих задач, от анализа настроений до предсказания нарциссизма из студенческих эссе. В случае прилагательных, описывающих личность, измерения векторных слов анализировались, назывались и обсуждались на протяжении десятилетий. Далее следует обсуждение различий на каждом этапе.

**Матрица частот.** ЛСА обычно включает большое количество разнообразных документов (например, миллионы отзывов о продуктах на Amazon). Они преобразуются в матрицу слов x документов путем подсчета, как часто каждое слово встречается в каждом документе. В психологии документом являются слова, которые субъект считает описывающими его. Это распространяется и на шкалы Лайкерта. Если кто-то говорит, что слово описывает его на 5/7, то просто повторите слово пять раз в документе.

**Нелинейное преобразование.** Лексические исследования часто ипсатизируют данные (z-оценка по оси субъекта), а затем выполняют корреляцию Пирсона. Терстоун использовал архаическую тетрархическую корреляцию в своем исследовании. В ЛСА наиболее распространенным преобразованием является TF-IDF, за которым следует логарифм. Это гарантирует, что матрица не будет доминирована общими терминами. Часто преобразование приводит к матрице близости слов x слов (например, корреляционной матрице). Этот шаг практически очень важен, но не столь теоретически значим. Преобразование, которое следует выбрать, — это то, которое дает разумный результат в конце.

**Разложение матрицы.** Существует множество методов разложения матриц. Некоторые, такие как PCA, требуют квадратной матрицы. Другие устойчивы к отсутствующим данным. В случае данных о личности выбор не имеет большого значения; результаты очень похожи. Общие векторные слова требуют ~300 измерений для представления значения слова, части речи, сленга и многого другого, что придает языку текстуру. Поскольку опросы разработаны для удержания большей части этого постоянным, требуется всего ~5 измерений. Терстоун обосновал свой выбор пяти, рассматривая ошибку реконструкции, которую он представляет в виде гистограммы. Позднее психологи обосновали 5 ошибкой реконструкции (измеренной с помощью собственных значений), а также учитывая интерпретируемость и воспроизводимость.

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!Zw-J!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd562c1c2-1576-4fad-896c-52e799d4598b_1600x1066.png)Ошибка реконструкции корреляционной матрицы слов. Несмотря на вычислительные ограничения, его выборка на порядок больше, чем у многих современных исследований.

**Вращение.** Вы когда-нибудь слышали о чрезмерной экстракции компонентов? Это не история, которую психологи расскажут вам. Это когда исследователь извлекает слишком много главных компонентов, а затем вращает дисперсию от более ранних, действительных ПК на более поздние, маргинальные ПК. Это произошло с Большой пятеркой, верите вы или нет! То, что сейчас называется Доброжелательностью, когда-то было гораздо более надежным и теоретически удовлетворительным фактором 'Социализации', который был распределен по ПК 3-5, чтобы создать Добросовестность, Нейротизм и Открытость. Вращение _может_ быть оправдано для получения интерпретируемых факторов. Но если вы когда-нибудь обнаружите, что вращаете и спорите о правильном количестве факторов, проверьте себя!

**Большая тройка из векторных слов**

Я начал свою докторскую диссертацию, предсказывая черты Большой пятерки из статусов Facebook. После прочтения о том, как создавалась "колбаса" личности, я понял, что проект использовал векторные слова (из статусов Facebook) для предсказания шумных приближений того, где люди находятся в пространстве Большой пятерки, которое изначально было определено векторными словами. Это казалось более интересным — перейти к сути и узнать что-то фундаментальное о личности из векторных слов. (Кроме того, набор данных, который я использовал, стал токсичным после Cambridge Analytica.) Остальная часть моей докторской работы была посвящена ограничению векторных слов для воспроизведения Большой пятерки. Это включало использование трансформеров вместо ЛСА (подробнее об этом в будущих постах). Полученная корреляция между факторами из векторных слов (DeBERTa) и опросами приведена ниже. Как вы можете видеть, существует очень близкое согласие для первых трех факторов. Там, где результаты расходятся, неясно, какой метод ошибочен. Возможно, опросы правы, и все корреляции достигнут 1, когда мы получим GPT-5. Возможно, опросы просто предвзяты и шумны, и было извлечено слишком много ПК. Возможно, они измеряют разные вещи, и нам нужно уточнить нашу интерпретацию обоих. В любом случае, мне не очевидно, что опросы следует считать золотым стандартом между двумя методами. Лексическая гипотеза касается структуры языка, в конце концов, и психология — единственная область, которая использует опросы для анализа естественного языка.

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!lY1-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6bf087b4-76cc-4272-a2f7-037d606ed2ba_726x682.png)Неротированные ПК из опросов взяты из одного из [исследований](https://onlinelibrary.wiley.com/doi/10.1002/\(SICI\)1099-0984\(199603\)10:1%3C61::AID-PER246%3E3.0.CO;2-D), которые определили Большую пятерку. ПК DeBERTa извлечены из векторных слов. Прочитайте об этом процессе [здесь](https://psyarxiv.com/gdm5v/).

**Заключение**

Терстоун был пионером в методах статистики и линейной алгебры для изучения Лексической гипотезы в 30-х годах. Удивительно, что он разработал способ представления слов, который позже был заново открыт для поиска информации, который теперь питает информационный век. Вычисления заставили Терстоуна упрощать богатый ландшафт языка до ответов на опросы. За последние 30 лет НЛП пережила несколько революций. Если Терстоун изобрел телескоп для изучения структуры языка, то сейчас у нас есть Хаббл. Множество инсайтов ждут!