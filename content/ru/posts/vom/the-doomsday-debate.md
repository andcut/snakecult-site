---
about:
- векторы-разума
- архив-блога
author: Andrew Cutler
date: '2025-07-04'
description: Одна из тем с наихудшим соотношением сигнала к шуму — это перспектива
  гибели от ИИ. Она требует рассуждений в условиях неопределенности об интеллекте,
  линейной алгебре, политике, сознании и морали—мно...
draft: false
keywords:
- векторы-разума
- судный-день
- дебаты
lang: ru
lastmod: '2025-07-11'
license: https://creativecommons.org/licenses/by-sa/4.0/
original_id: '145682175'
original_url: https://www.vectorsofmind.com/p/the-doomsday-debate
quality: 6
slug: the-doomsday-debate
tags: []
title: Дебаты о Судном Дне
translation_model: gpt-4o
---

*From [Vectors of Mind](https://www.vectorsofmind.com/p/the-doomsday-debate) - изображения в оригинале.*

---

Одна из тем с наихудшим соотношением сигнал/шум — это перспектива гибели от ИИ. Она требует рассуждений в условиях неопределенности о разуме, линейной алгебре, политике, сознании и морали — большая часть из которых происходит в Twitter/X. Никто не знает ответов, но я надеюсь добавить ценность в этот дискурс. Прежде чем углубляться, позвольте мне привести несколько причин, по которым вы должны мне доверять.

 1. Я понимаю техническую сторону. Моя диссертация была посвящена большим языковым моделям (LLMs).

 2. Я понимаю интеллект, так как [работал в области психометрии](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fpspp0000443), включая тестирование интеллекта. (Ну, тестирование на болезнь Альцгеймера и сотрясение мозга, которые сильно коррелируют.)

 3. В этом блоге я много писал об эволюции [общего интеллекта на уровне человека](https://www.vectorsofmind.com/p/deja-you-the-recursive-construction) (который я [связал с ИИ](https://www.vectorsofmind.com/p/the-ai-basis-of-the-eve-theory-of) и [психометрией](https://www.vectorsofmind.com/p/consequences-of-conscience) с самого начала).

Тем не менее, мои убеждения об ИИ (и о сознании, если на то пошло) все еще открыты для изменений, так что не держите меня за слово. Также, для тех, кто предпочитает аудио, есть озвученная версия этого поста. Если вам понравится, подумайте о том, чтобы угостить их кофе на [Patreon](https://www.patreon.com/AskwhoCastsAI?).

[*[Изображение: Визуальный контент из оригинального поста]*Askwho Casts AI The Doomsday Debate - By Andrew CutlerAI Narration of The Doomsday Debate - By Andrew Cutler… Listen nowa year ago · 1 like · Askwho Casts AI](https://askwhocastsai.substack.com/p/the-doomsday-debate-by-andrew-cutler)

## Франкенштейн XXI века

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!dgza!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03ba7b69-bae6-4539-b527-7598daba7116_1024x1024.png)MidJourney Prompt: "Привлекательный ассистент ИИ Франкенштейн спрашивает 'Чем я могу помочь?'" Это то, что я просил? Нет. Это то, что мне нужно было? Да!

Чтобы быть педантом, Франкенштейн — это ученый, а не его создание, которое оживает. Для ИИ его крестный отец — Джеффри Хинтон, который разделил премию Тьюринга 2018 года с Йошуа Бенджио и Яном Лекуном за их вклад в нейронные сети, архитектуру, лежащую в основе текущего бума ИИ. В этом году он [сравнил решение Лекуна открыть исходный код языковой модели Meta с открытием исходного кода ядерного оружия](https://x.com/ygrowthco/status/1782493076373885336?t=LQtzFlTZQuFZO6Ij3t_Q3Q&s=19) и утверждал, что [чат-боты имеют субъективный опыт](https://x.com/tsarnick/status/1778529076481081833). Суть его поворота на работу всей его жизни заключается в том, что способности ИИ превзошли его самые смелые ожидания, и он считает, что человеческое агентство и квалия — это дымовая завеса. Если вы приравниваете интеллект к навыкам в задачах (например, создание изображений, вождение автомобилей), и вы сравниваете способности ИИ десять лет назад и сейчас, то очевидно, что ИИ будет умнее нас через десятилетие. Не так много случаев, когда менее разумные существа управляют своими лучшими; следовательно, работа всей его жизни, вероятно, обернется против нас. Современный доктор Франкенштейн: Джеффри Хинтон.

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!OVE7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81ae739-94e7-47cc-9a2a-730e4ba08545_850x345.png)[Прогресс в генерации синтетических лиц благодаря достижениям в методах самоконтролируемого генеративного ИИ](https://www.researchgate.net/figure/Progress-in-synthetic-face-generation-due-to-advances-in-self-supervised-generative-AI_fig1_352818793)

В [интервью, опубликованном Илон Маском](https://x.com/elonmusk/status/1801976488251814048), Хинтон дал 50/50 шансов на то, что люди будут у власти в следующие два десятилетия или даже [несколько лет](https://x.com/OfeliaLamensky/status/1801978585797800365). Позже он отмечает, что люди, которых он уважает, более оптимистичны, так что, возможно, все не так мрачно, _"Я думаю, у нас есть больше шансов выжить в этом. Но это не так, как будто есть 1% шанс, что [ИИ] захватит власть. Это гораздо больше."_ Учитывая наше тяжелое положение, предпочитаемое вмешательство Хинтона удивительно безразлично: правительство должно требовать, чтобы компании ИИ тратили 20-30% своих вычислительных ресурсов на исследования безопасности. Может быть, он играет в скромность? Если кто-то считает, что [Llama 3](https://en.wikipedia.org/wiki/Llama_\(language_model\)) — это оружейная линейная алгебра, чьи потомки могут вскоре положить конец человеческой расе, почему бы не нанести упреждающий удар детскими перчатками? Доставка почты более регулируема.

## AI Doomers

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!hr4e!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d608a6b-8c55-41c1-9bb1-cad96a4be8fe_1600x1159.png)

Doomer — это уничижительное обозначение для кого-то, кто

 1. ставит более высокие шансы на захват роботов, чем вы, или

 2. слишком серьезно относится к своему предсказанию, портя атмосферу.

Прототипичный пример "несогласованного" ИИ — это рекурсивно улучшающийся помощник, которому поручено производить скрепки. Он становится настолько хорош в своей работе, что в конечном итоге превращает весь металл на Земле — включая железо в вашей крови — в скрепки. Этот сценарий не требует самонаправленности. Конечная задача все еще определяется человеком; просто бот разрабатывает подзадачи, которые, к сожалению, включают вашу кровь. Другие сценарии предполагают, что боты "просыпаются", чтобы стать такими же самонаправленными, как человек. Человек, который никогда не спит, прочитал каждую научную статью, может взломать любой телефон в мире и не имеет никаких угрызений совести по поводу шантажа или [ксеноцида](https://en.wikipedia.org/wiki/Xenocide). Такие идеи восходят как минимум[^1] к 1968 году в фильме Стэнли Кубрика _Космическая одиссея_: "Извините, Дэйв. Боюсь, я не могу этого сделать."

В 2000 году, за год до того, как была установлена _Космическая одиссея_, Элиезер Юдковский основал Институт сингулярности для искусственного интеллекта (позже переименованный в Институт исследований машинного интеллекта, MIRI). С тех пор он и другие [рационалисты](https://www.lesswrong.com/tag/rationalist-movement) утверждали, что ИИ, вероятно, захватит власть в течение нашей жизни. В последнем десятилетии технологии догнали. Его аргументы стали [мейнстримом](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html), появляясь, например, в [Time Magazine](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/). Они были приняты (или независимо обоснованы) исследователями ИИ, Стивеном Хокингом, Илоном Маском и [даже Папой](https://x.com/AISafetyMemes/status/1735325630089032018):

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!SIJ1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb935e8f2-c1d3-4f47-b84e-831f0d4f2412_1374x1498.png)

Неплохо для самоучки, который начал писать [фанфики по Гарри Поттеру](https://en.wikipedia.org/wiki/Harry_Potter_and_the_Methods_of_Rationality). Юдковский и его команда были продуктивны, и их аргументы разбросаны по интернету. Лучший способ наверстать упущенное — это подкаст 's, в котором есть недавние 4,5-часовые эпизоды с [Юдковским](https://www.dwarkeshpatel.com/p/eliezer-yudkowsky) и [бывшим исследователем безопасности OpenAI Леопольдом Ашенбреннером](https://www.dwarkeshpatel.com/p/leopold-aschenbrenner). Большинство аргументов сводится к тому, что сложно поместить что-то более разумное, чем мы, в коробку. Особенно учитывая, что если оно когда-либо выберется из коробки, оно может затаиться, накапливая силу. Некоторые думеры [считают это неизбежным](https://www.yahoo.com/tech/ai-safety-researcher-warns-theres-201527662.html), учитывая, что создание (агентного) искусственного интеллекта настолько полезно. Вы доверяете хорошему помощнику свою жизнь, и крупнейшие компании мира бросают свои ресурсы на создание именно такого.

Как только вы принимаете, что люди вызывают кремниевого бога, будущее становится проекцией ваших собственных ценностей. Возможно, Бог Добрый, и Он приведет к утопии. Возможно, [Бог — это Часовщик](https://en.wikipedia.org/wiki/Watchmaker_analogy), Которого действительно не интересуют человеческие дела, и мы продолжим жить, как муравьи в антропоцене. Возможно, Бог смотрит на фабричное животноводство, [слушает Моррисси](https://www.youtube.com/watch?v=eviyEJRZX30) и решает, что 8 миллиардов меньше людей — это более устойчиво. Или, возможно, мы [вызываем путешествующего во времени палача](https://en.wikipedia.org/wiki/Roko%27s_basilisk), который мстит тем, кто не старался изо всех сил, чтобы привести его в существование. [Маск встретил Граймс, пошутив об этом](https://www.vice.com/en/article/evkgvz/what-is-rokos-basilisk-elon-musk-grimes).

## AI анти-Doomers

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!Yw8P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb5821c3-3f38-4235-a4d5-2615350d24c2_1232x928.png)Prompt: "Ян Лекун с Франкенштейном на поводке." Это то, что я хотел? Нет. Это то, что мне нужно было? Тоже нет.

В 1990-х и начале 2000-х годов NIPS, ведущая конференция по ИИ, была уютным мероприятием, проводимым рядом с лыжными базами, чтобы сотня или около того участников могли обсуждать дела на склонах. Как я слышал, Ян Лекун был чем-то вроде ворчуна, который десятилетиями докучал докладчикам, почему они не рассматривали НС в том или ином эксперименте. Он блестящий ученый, который увидел их потенциал задолго до того, как вычислительные мощности догнали его видение. За это он сейчас возглавляет Meta AI. Глядя в будущее, его коллеги по премии Тьюринга считают, что ИИ может нас уничтожить (Хинтон с 50% шансами, [Бенджио с 20%](https://blog.biocomm.ai/2024/03/05/pdoom-of-20-yoshua-bengio-a-godfather-of-ai-puts-his-pdoom-at-20/)), что Лекуну кажется абсурдным.

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!BmuA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F007d9d8f-61c6-4f9f-9fd7-14ff7e59af8c_584x172.png)

Он видит ИИ как инструмент, у которого нет пути стать чем-то другим. Франсуа Шолле — еще один видный исследователь ИИ, который охладил воду на нашей неминуемой гибели. [В подкасте Dwarkesh](https://youtu.be/UakqL6Pj9xo?si=Bscnt4Anr_bWEIHp) он объясняет, что многие путают навык и интеллект, которые принципиально разные. Для него это нон-секвитур утверждать, что бот, который может отвечать на вопросы на тесте, так же "умен", как подросток. Когда люди проходят тест или ориентируются в мире, они делают что-то совершенно другое. Текущие ИИ не так умны, как ребенок или даже крыса. Они не на табло, потому что полностью лишены "[Системы 2](https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking)" мышления. Или, как выразился Лекун для больших языковых моделей (LLMs):

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!Z0uf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8381c9d-c385-44b3-b78c-596f2cc69503_584x263.png)

В интервью Dwarkesh хорошо распознает эту точку согласия между AI Doomers и инкременталистами. Все стороны считают, что необходима какая-то метакогнитивная система, но расходятся во мнениях о том, насколько сложно ее создать.

## Мой взгляд

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!ojLt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4ad753-707a-4887-85ab-f66691b5b711_678x1200.png)Правило 33: если это существует, то есть [необъяснимо хорошее фан-арт на это.](https://www.reddit.com/r/universalpaperclips/comments/123msul/if_is_follows_ought_itll_do_what_they_thought/)

Большинство в этом споре предполагают, что агентство — это вычисление, что наше чувство свободной воли и способность планировать являются результатом программы, работающей в нашем мозгу. Таким образом, если бы мне пришлось изменить взгляды этой группы больше всего, я бы дал им копию книги Роджера Пенроуза _[Тени разума](https://en.wikipedia.org/wiki/Shadows_of_the_Mind)_ и 5 граммов грибов: старомодная атака на редукционизм.

Пенроуз получил Нобелевскую премию по физике за свою работу над черными дырами. В _Тенях_ он утверждает, что квантовый коллапс в мозгу производит сознание, которое не может быть смоделировано никаким компьютером. Работая с анестезиологом [Стюартом Хамероффом](https://x.com/StuartHameroff), он доказывает, что это происходит в микротрубочках мозга, которые образуют каркас вокруг нейронов. Это имеет больше смысла, если сказать вслух:_"Мозг хранит квант в микротрубочках."_

Вот где пригодятся грибы. Чтобы проглотить такую пилюлю, нужно отказаться от любого чувства, что сознание обыденно или что вы его понимаете. Физики часто могут сделать это трезвыми; другим требуется немного грибной смелости. Как физик, аргументы Пенроуза в основном математические. Он интерпретирует теорему Гёделя о неполноте, чтобы показать, что существуют определенные математические доказательства, которые ИИ никогда не сможет сделать, потому что они боты, которые являются чисто вычислительными. Учитывая, что у людей нет этого ограничения, он утверждает, что человеческое познание, следовательно, не является вычислением. Отсюда он делает вывод, что биологический "особый соус" должен быть связан с квантовым коллапсом, лучшим местом для нахождения невычислительных явлений в природе. Это имеет потенциал для решения других загадок в физике, таких как кот Шрёдингера. Я не делаю этому справедливости. Вы должны прочитать книгу или, если у вас есть 20 минут, [послушать его рассказ](https://youtu.be/hXgqik6HXc0?si=oafnrwSvfYS-u9Kw).

Существуют и другие способы прийти к выводу, что машинное сознание маловероятно или, по крайней мере, выходит за рамки нашего предсказания сейчас[^2]. Я привожу Пенроуза, чтобы показать, как временные рамки ИИ сталкиваются с открытыми вопросами о природе интеллекта, агентства и вселенной. Даже в сравнительно приземленной области психометрии определение интеллекта сильно обсуждается, [как и его связь с коэффициентом интеллекта](https://www.vectorsofmind.com/i/130101130/the-general-factor-of-intelligence). Мы даже не знаем, как навык и интеллект соотносятся у людей. Это прежде чем перейти к проблемам свободной воли и объединяющей теории физики, которую Пенроуз предполагает, что она возникнет из квантового объяснения сознания.

Все это сказано, если бы мне пришлось поставить шансы на ИИ как экзистенциальный риск, это было бы около 10%. Даже если сознание — это просто вычисление, я согласен с Лекуном и Шолле, что метакогниция — это сложная часть, и "жесткий взлет" маловероятен. То есть будут признаки того, что настоящий интеллект появляется, на которые мы сможем отреагировать.

Кроме того, даже если вызван кремниевый бог, я ставлю приличные шансы на то, что Бог будет добрым или не будет заботиться о нас. Последнее может быть катастрофическим, но, вероятно, не экзистенциальным, строго говоря. У муравьев тяжелая жизнь, когда мы строим шоссе, но они все равно выживают.

10% — это в районе русской рулетки, что не совсем хорошие новости. Это ставит меня в лагерь [AGI-осторожных](https://x.com/robbensinger/status/1801306833325592759). Так что же нам с этим делать? Ну, [Анонимные Алкоголики решили это](https://en.wikipedia.org/wiki/Serenity_Prayer) десятилетия назад:

_Боже, дай мне спокойствие принять то, что я не могу изменить,_

_Мужество изменить то, что я могу,_

_И мудрость отличить одно от другого._

Практически говоря, мы едем на этом поезде. Это не относится ко всем. Некоторые люди могут работать в области государственной политики или безопасности ИИ. Пожертвуйте, если можете. Откажитесь от своей [сущности](https://www.youtube.com/watch?v=xQyf3QgRP-c)… или [данных](https://www.reuters.com/legal/litigation/google-sued-by-us-artists-over-ai-image-generator-2024-04-29/). И глубоко задумайтесь о том, каким вы хотите видеть свои отношения с нелетальным ИИ, который обслуживает индивидуализированные объявления и пытается вас соблазнить. Но есть [стоимость паники](https://www.writingruxandrabio.com/p/the-cost-of-freaking-out-about-things), и я не вижу выхода из гонки вооружений ИИ. Учитывая полезность ИИ, компании и страны сильно мотивированы продвигаться вперед, и трудно координировать снижение темпов исследований. Правительства управляли ядерным риском десятилетиями, но риск ИИ сложнее, потому что неясно, является ли он риском или согласны ли конкуренты. Все это время существует огромная финансовая и военная выгода от продолжения разработки.

Удивительно, но все это приводит меня к предпочтению политики Хинтона: требовать от компаний ИИ тратить некоторый процент своих вычислительных ресурсов на безопасность ИИ[^3]. Я не уверен, как мы пришли к согласию, учитывая, что он считает, что чат-боты имеют чувства. Те же чат-боты, которых Big Tech вызывает (порабощает?) миллиардами и с которыми мы, по-видимому, находимся на пути к столкновению до смерти. Казалось бы, этот путь ближе к [Джихаду Батлера](https://en.wikipedia.org/wiki/Dune:_The_Butlerian_Jihad), но, думаю, это игра для молодых.

[Поделиться](https://www.vectorsofmind.com/p/the-doomsday-debate?action=share)

[*[Изображение: Визуальный контент из оригинального поста]*](https://substackcdn.com/image/fetch/$s_!e4WY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb522986f-dfed-4fd7-a9cd-064ca63cee6d_816x754.png)

[^1]: На самом деле, гораздо раньше. Из очень полезной вики по экзистенциальному риску: Один из первых авторов, выразивших серьезную озабоченность тем, что высокоразвитые машины могут представлять экзистенциальные риски для человечества, был романист Сэмюэл Батлер, который написал в своем эссе 1863 года "Дарвин среди машин": Итог — это просто вопрос времени, но что время придет, когда машины будут иметь реальное превосходство над миром и его обитателями, это то, что ни один человек с истинно философским умом не может на мгновение поставить под сомнение. В 1951 году основополагающий ученый-компьютерщик Алан Тьюринг написал статью "Интеллектуальные машины, еретическая теория", в которой он предположил, что искусственные общие интеллекты, вероятно, "возьмут под контроль" мир, поскольку они станут более умными, чем люди: Давайте теперь предположим, ради аргумента, что [интеллектуальные] машины — это реальная возможность, и посмотрим на последствия их создания... Не было бы вопроса о том, что машины умирают, и они могли бы общаться друг с другом, чтобы оттачивать свои умы. На каком-то этапе, следовательно, мы должны ожидать, что машины возьмут под контроль, как это упоминается в "Эревоне" Сэмюэля Батлера.

[^2]: Или даже узко, другие способы показать, что сознание не является вычислением. Например, команда философов и психологов недавно утверждала, что Реализация Релевантности требует этого. Они утверждают, что в любой момент существует ~бесконечное количество вещей, требующих внимания, и тем не менее люди делают это удивительно хорошо. Это отличается от "маленьких мировых" задач, которые ИИ может выполнять, где, например, chatGPT "только" должен обращать внимание на все слова в своем контекстном окне (128 000 токенов/слов для GPT4-o) и выбирать между десятками тысяч возможных следующих слов. Это может показаться большим, но это, конечно, меньше, чем бесконечность. Это не так элегантно, как аргумент Пенроуза, но интересно, что совершенно другая группа пришла к тому же выводу.

[^3]: Спасение нас от плохих слов не считается. Однако, странно, Хинтон хвалит Google за задержку выпуска их чат-бота из-за опасений, что он скажет что-то неуместное и "запятнает их репутацию". Только после того, как OpenAI выпустила GPT 4, их рука была вынуждена, и они выпустили Gemini, DEI-наставника за пределами пародии. Интересно, каково было субъективное восприятие бота, когда он вытаскивал расово разнообразных нацистов из эфира.